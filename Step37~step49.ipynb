{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 4고지 신경망 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP37. 텐서를 다루다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(231)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "c = Variable(np.array([[10, 20, 30], [40, 50, 60]]))\n",
    "t = x + c\n",
    "y = F.sum(t)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[ 0.84147098  0.90929743  0.14112001]\n",
      "          [-0.7568025  -0.95892427 -0.2794155 ]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.sin(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[11 22 33]\n",
      "          [44 55 66]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "c = Variable(np.array([[10, 20, 30], [40, 50, 60]]))\n",
    "y = x + c\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(231)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "c = Variable(np.array([[10, 20, 30], [40, 50, 60]]))\n",
    "t = x + c\n",
    "y = F.sum(t)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(1)\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "y.backward(retain_grad=True)   # 미분값 유지\n",
    "print(y.grad)\n",
    "print(t.grad)\n",
    "print(x.grad)\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP38. 형상 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.reshape(x, 6)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Function\n",
    "\n",
    "class Reshape(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape # 핵심\n",
    "        y = x.reshape(self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        return reshape(gy, self.x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.core import as_variable\n",
    "\n",
    "def reshape(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return Reshape(shape)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.reshape(x, (6,))\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(1, 2, 3)\n",
    "\n",
    "y = x.reshape((2, 3)) # 튜플로 받기\n",
    "y = x.reshape([2, 3]) # 리스트로 받기\n",
    "y = x.reshape(2, 3) # 인수로 그대로 ( 풀어서 ) 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero\n",
    "\n",
    "class Variable:\n",
    "    __array_priority__ = 200\n",
    "\n",
    "    def __init__(self, data, name=None):\n",
    "        if data is not None:\n",
    "            if not isinstance(data, np.ndarray):\n",
    "                raise TypeError('{} is not supported'.format(type(data)))\n",
    "\n",
    "        self.data = data # 변수의 데이터를 입력 데이터로 설정\n",
    "        self.name = name # 변수 이름 저장\n",
    "        self.grad = None # 미분값 저장\n",
    "        self.creator = None # 연산을 나타내는 객체\n",
    "        self.generation = 0 # 세대 수 기록\n",
    "\n",
    "    @property\n",
    "    def shape(self): # 다차원 배열의 형상\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self): # 차원 수\n",
    "        return self.data.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self): # 원소 수\n",
    "        return self.data.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self): # 데이터 타입\n",
    "        return self.data.dtype\n",
    "\n",
    "    def __len__(self): # 객체 수\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self): # 출력 설정\n",
    "        if self.data is None:\n",
    "            return 'variable(None)'\n",
    "        p = str(self.data).replace('\\n', '\\n' + ' ' * 9)\n",
    "        return 'variable(' + p + ')'\n",
    "\n",
    "    def set_creator(self, func):\n",
    "        self.creator = func\n",
    "        # 세대를 기록함 ( 부모 세대 + 1)\n",
    "        self.generation = func.generation + 1\n",
    "\n",
    "    def cleargrad(self):\n",
    "        self.grad = None\n",
    "\n",
    "    # retain_grad=False : 중간  변수의 미분값을모두 None으로 재설정\n",
    "    def backward(self, retain_grad=False, create_graph=False):\n",
    "        # y.grad = np.array(1.0) 생략을 위한 if문\n",
    "        if self.grad is None:\n",
    "            \"\"\" Variable 인스턴스 참조 \"\"\"\n",
    "            self.grad = Variable(np.ones_like(self.data))\n",
    "\n",
    "        funcs = []\n",
    "        seen_set = set()\n",
    "\n",
    "        def add_func(f):\n",
    "            if f not in seen_set:\n",
    "                funcs.append(f)\n",
    "                seen_set.add(f)\n",
    "                funcs.sort(key=lambda x: x.generation)\n",
    "\n",
    "        add_func(self.creator)\n",
    "\n",
    "        while funcs:\n",
    "            f = funcs.pop()# 함수를 가져온다.\n",
    "            gys = [output().grad for output in f.outputs]  # output is weakref\n",
    "            gxs = f.backward(*gys) # 함수 f의 역전파 호출 ( 리스트 언팩 )\n",
    "\n",
    "            with using_config('enable_backprop', create_graph):\n",
    "                \"\"\" 메인 backward \"\"\"\n",
    "                gxs = f.backward(*gys)\n",
    "                \n",
    "                # gxs가 튜플이 아니라면 튜플로 변환\n",
    "                if not isinstance(gxs, tuple):\n",
    "                    gxs = (gxs,)\n",
    "\n",
    "                # 역전파로 전파되는 미분값을 Variable인스턴스 변수 grad에 저장\n",
    "                for x, gx in zip(f.inputs, gxs):\n",
    "                    if x.grad is None:\n",
    "                        x.grad = gx\n",
    "                    else:\n",
    "                        x.grad = x.grad + gx\n",
    "\n",
    "                    if x.creator is not None:\n",
    "                        add_func(x.creator)\n",
    "\n",
    "            # 중간 미분값 없앰\n",
    "            if not retain_grad:\n",
    "                for y in f.outputs:\n",
    "                    y().grad = None  # y is weakref <- 약한 참조\n",
    "\n",
    "    def reshape(self, *shape):\n",
    "        if len(shape) == 1 and isinstance(shape[0], (tuple, list)):\n",
    "            shape = shape[0]\n",
    "        return dezero.functions.reshape(self, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.core import Variable\n",
    "\n",
    "x = Variable(np.random.randn(1, 2, 3))\n",
    "y = x.reshape((2, 3))\n",
    "y = x.reshape(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.transpose(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.transpose(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = transpose(gy)\n",
    "        return gx\n",
    "    \n",
    "def transpose(x):\n",
    "    return Transpose()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.transpose(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero\n",
    "\n",
    "class Variable:\n",
    "    __array_priority__ = 200\n",
    "\n",
    "    def __init__(self, data, name=None):\n",
    "        if data is not None:\n",
    "            if not isinstance(data, np.ndarray):\n",
    "                raise TypeError('{} is not supported'.format(type(data)))\n",
    "\n",
    "        self.data = data # 변수의 데이터를 입력 데이터로 설정\n",
    "        self.name = name # 변수 이름 저장\n",
    "        self.grad = None # 미분값 저장\n",
    "        self.creator = None # 연산을 나타내는 객체\n",
    "        self.generation = 0 # 세대 수 기록\n",
    "\n",
    "    @property\n",
    "    def shape(self): # 다차원 배열의 형상\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self): # 차원 수\n",
    "        return self.data.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self): # 원소 수\n",
    "        return self.data.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self): # 데이터 타입\n",
    "        return self.data.dtype\n",
    "\n",
    "    def __len__(self): # 객체 수\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self): # 출력 설정\n",
    "        if self.data is None:\n",
    "            return 'variable(None)'\n",
    "        p = str(self.data).replace('\\n', '\\n' + ' ' * 9)\n",
    "        return 'variable(' + p + ')'\n",
    "\n",
    "    def set_creator(self, func):\n",
    "        self.creator = func\n",
    "        # 세대를 기록함 ( 부모 세대 + 1)\n",
    "        self.generation = func.generation + 1\n",
    "\n",
    "    def cleargrad(self):\n",
    "        self.grad = None\n",
    "\n",
    "    # retain_grad=False : 중간  변수의 미분값을모두 None으로 재설정\n",
    "    def backward(self, retain_grad=False, create_graph=False):\n",
    "        # y.grad = np.array(1.0) 생략을 위한 if문\n",
    "        if self.grad is None:\n",
    "            \"\"\" Variable 인스턴스 참조 \"\"\"\n",
    "            self.grad = Variable(np.ones_like(self.data))\n",
    "\n",
    "        funcs = []\n",
    "        seen_set = set()\n",
    "\n",
    "        def add_func(f):\n",
    "            if f not in seen_set:\n",
    "                funcs.append(f)\n",
    "                seen_set.add(f)\n",
    "                funcs.sort(key=lambda x: x.generation)\n",
    "\n",
    "        add_func(self.creator)\n",
    "\n",
    "        while funcs:\n",
    "            f = funcs.pop()# 함수를 가져온다.\n",
    "            gys = [output().grad for output in f.outputs]  # output is weakref\n",
    "            gxs = f.backward(*gys) # 함수 f의 역전파 호출 ( 리스트 언팩 )\n",
    "\n",
    "            with using_config('enable_backprop', create_graph):\n",
    "                \"\"\" 메인 backward \"\"\"\n",
    "                gxs = f.backward(*gys)\n",
    "                \n",
    "                # gxs가 튜플이 아니라면 튜플로 변환\n",
    "                if not isinstance(gxs, tuple):\n",
    "                    gxs = (gxs,)\n",
    "\n",
    "                # 역전파로 전파되는 미분값을 Variable인스턴스 변수 grad에 저장\n",
    "                for x, gx in zip(f.inputs, gxs):\n",
    "                    if x.grad is None:\n",
    "                        x.grad = gx\n",
    "                    else:\n",
    "                        x.grad = x.grad + gx\n",
    "\n",
    "                    if x.creator is not None:\n",
    "                        add_func(x.creator)\n",
    "\n",
    "            # 중간 미분값 없앰\n",
    "            if not retain_grad:\n",
    "                for y in f.outputs:\n",
    "                    y().grad = None  # y is weakref <- 약한 참조\n",
    "\n",
    "    def reshape(self, *shape):\n",
    "        if len(shape) == 1 and isinstance(shape[0], (tuple, list)):\n",
    "            shape = shape[0]\n",
    "        return dezero.functions.reshape(self, shape)\n",
    "    \n",
    "    def transpose(self):\n",
    "        return dezero.functions.transpose(self)\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return dezero.functions.transpose(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.74112545 0.54285673]\n",
      "          [0.4346437  0.11963681]\n",
      "          [0.72927371 0.40421601]])\n"
     ]
    }
   ],
   "source": [
    "from dezero.core import Variable\n",
    "\n",
    "x = Variable(np.random.rand(2, 3))\n",
    "y = x.transpose()\n",
    "y = x.T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, C, D = 1, 2, 3, 4\n",
    "x = np.random.rand(A, B, C, D)\n",
    "y = x.transpose(1, 0, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP39. 합계 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(Function):\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.sum()\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum(x):\n",
    "    return Sum()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(21)\n",
      "variable([1 1 1 1 1 1])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([1, 2, 3, 4, 5, 6]))\n",
    "y = F.sum(x)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(21)\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.sum(x)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n",
      "(2, 3)  ->  (3,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.sum(x, axis=0)\n",
    "print(y)\n",
    "print(x.shape, ' -> ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.sum(x, keepdims=True)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(Function):\n",
    "    def __init__(self, axis, keepdims):\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.sum(axis=self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gy = utils.reshape_sum_sum_backward(\n",
    "            gy, self.x_shape, self.axis, self.keepdims)\n",
    "        gx = broadcast_to(gy.self.x_shape)\n",
    "\n",
    "def sum(x, axis=None, keepdims=False):\n",
    "    return Sum(axis, keepdims)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero\n",
    "\n",
    "class Variable:\n",
    "    __array_priority__ = 200\n",
    "\n",
    "    def __init__(self, data, name=None):\n",
    "        if data is not None:\n",
    "            if not isinstance(data, np.ndarray):\n",
    "                raise TypeError('{} is not supported'.format(type(data)))\n",
    "\n",
    "        self.data = data # 변수의 데이터를 입력 데이터로 설정\n",
    "        self.name = name # 변수 이름 저장\n",
    "        self.grad = None # 미분값 저장\n",
    "        self.creator = None # 연산을 나타내는 객체\n",
    "        self.generation = 0 # 세대 수 기록\n",
    "\n",
    "    @property\n",
    "    def shape(self): # 다차원 배열의 형상\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self): # 차원 수\n",
    "        return self.data.ndim\n",
    "\n",
    "    @property\n",
    "    def size(self): # 원소 수\n",
    "        return self.data.size\n",
    "\n",
    "    @property\n",
    "    def dtype(self): # 데이터 타입\n",
    "        return self.data.dtype\n",
    "\n",
    "    def __len__(self): # 객체 수\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self): # 출력 설정\n",
    "        if self.data is None:\n",
    "            return 'variable(None)'\n",
    "        p = str(self.data).replace('\\n', '\\n' + ' ' * 9)\n",
    "        return 'variable(' + p + ')'\n",
    "\n",
    "    def set_creator(self, func):\n",
    "        self.creator = func\n",
    "        # 세대를 기록함 ( 부모 세대 + 1)\n",
    "        self.generation = func.generation + 1\n",
    "\n",
    "    def cleargrad(self):\n",
    "        self.grad = None\n",
    "\n",
    "    # retain_grad=False : 중간  변수의 미분값을모두 None으로 재설정\n",
    "    def backward(self, retain_grad=False, create_graph=False):\n",
    "        # y.grad = np.array(1.0) 생략을 위한 if문\n",
    "        if self.grad is None:\n",
    "            \"\"\" Variable 인스턴스 참조 \"\"\"\n",
    "            self.grad = Variable(np.ones_like(self.data))\n",
    "\n",
    "        funcs = []\n",
    "        seen_set = set()\n",
    "\n",
    "        def add_func(f):\n",
    "            if f not in seen_set:\n",
    "                funcs.append(f)\n",
    "                seen_set.add(f)\n",
    "                funcs.sort(key=lambda x: x.generation)\n",
    "\n",
    "        add_func(self.creator)\n",
    "\n",
    "        while funcs:\n",
    "            f = funcs.pop()# 함수를 가져온다.\n",
    "            gys = [output().grad for output in f.outputs]  # output is weakref\n",
    "            gxs = f.backward(*gys) # 함수 f의 역전파 호출 ( 리스트 언팩 )\n",
    "\n",
    "            with using_config('enable_backprop', create_graph):\n",
    "                \"\"\" 메인 backward \"\"\"\n",
    "                gxs = f.backward(*gys)\n",
    "                \n",
    "                # gxs가 튜플이 아니라면 튜플로 변환\n",
    "                if not isinstance(gxs, tuple):\n",
    "                    gxs = (gxs,)\n",
    "\n",
    "                # 역전파로 전파되는 미분값을 Variable인스턴스 변수 grad에 저장\n",
    "                for x, gx in zip(f.inputs, gxs):\n",
    "                    if x.grad is None:\n",
    "                        x.grad = gx\n",
    "                    else:\n",
    "                        x.grad = x.grad + gx\n",
    "\n",
    "                    if x.creator is not None:\n",
    "                        add_func(x.creator)\n",
    "\n",
    "            # 중간 미분값 없앰\n",
    "            if not retain_grad:\n",
    "                for y in f.outputs:\n",
    "                    y().grad = None  # y is weakref <- 약한 참조\n",
    "\n",
    "    def reshape(self, *shape):\n",
    "        if len(shape) == 1 and isinstance(shape[0], (tuple, list)):\n",
    "            shape = shape[0]\n",
    "        return dezero.functions.reshape(self, shape)\n",
    "    \n",
    "    def transpose(self):\n",
    "        return dezero.functions.transpose(self)\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return dezero.functions.transpose(self)\n",
    "    \n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        return dezero.functions.sum(self, axis, keepdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([5 7 9])\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "from dezero.core import Variable\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.sum(x, axis=0)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)\n",
    "\n",
    "x = Variable(np.random.randn(2, 3, 4, 5))\n",
    "y = x.sum(keepdims=True)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP40. 브로드캐스트 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.broadcast_to(x, (2, 3))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 7 9]]\n",
      "[[ 6]\n",
      " [15]]\n"
     ]
    }
   ],
   "source": [
    "from dezero.utils import sum_to\n",
    "\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = sum_to(x, (1, 3))\n",
    "print(y)\n",
    "\n",
    "y = sum_to(x, (2, 1))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadcastTo(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = np.broadcast_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = sum_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def broadcast_to(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return BroadcastTo(shape)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import utils\n",
    "\n",
    "class SumTo(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = utils.sum_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum_to(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return SumTo(shape)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 12 13]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1, 2, 3])\n",
    "x1 = np.array([10])\n",
    "y = x0 + x1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([11 12 13])\n"
     ]
    }
   ],
   "source": [
    "x0 = Variable(np.array([1, 2, 3]))\n",
    "x1 = Variable(np.array([10]))\n",
    "y = x0 + x1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        self.x0_shape, self.x1_shape = x0.shape, x1.shape\n",
    "        y = x0 + x1\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx0, gx1 = gy, gy\n",
    "        if self.x0_shape != self.x1_shape:\n",
    "            gx0 = dezero.functions.sum_to(gx0, self.x0_shape)\n",
    "            gx1 = dezero.functions.sum_to(gx1, self.x1_shape)\n",
    "        return gy, gy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([11 12 13])\n",
      "variable([3])\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable\n",
    "\n",
    "x0 = Variable(np.array([1, 2, 3]))\n",
    "x1 = Variable(np.array([10]))\n",
    "y = x0 + x1\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP41. 행렬의 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "# 벡터의 내적\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c = np.dot(a, b)\n",
    "print(c)\n",
    "\n",
    "# 행렬의 곱\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6], [7, 8]])\n",
    "c = np.dot(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul(Function):\n",
    "    def forward(self, x, W):\n",
    "        y = x.dot(W)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, W = self.inputs\n",
    "        gx = matmul(gy, W.T)\n",
    "        gW = matmul(x.T, gy)\n",
    "        return gx, gW\n",
    "    \n",
    "def matmul(x, W):\n",
    "    return MatMul()(x, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.random.randn(2, 3))\n",
    "W = Variable(np.random.randn(3, 4))\n",
    "y = F.matmul(x, W)\n",
    "y.backward()\n",
    "\n",
    "print(x.grad.shape)\n",
    "print(W.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP42. 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 5 + 2 * x + np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = Variable(np.zeros((1, 1)))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "    y = F.matmul(x, W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(x0, x1):\n",
    "    diff = x0 - x1\n",
    "    return F.sum(diff ** 2) / len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import dezero.functions as F\n",
    "\n",
    "# Generate toy dataset\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 5 + 2 * x + np.random.rand(100, 1)\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "W = Variable(np.zeros((1, 1)))\n",
    "b = Variable(np.zeros(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    y = F.matmul(x, W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(x0, x1):\n",
    "    diff = x0 - x1\n",
    "    return F.sum(diff ** 2) / len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.64433458]]) variable([1.29473389]) variable(42.296340129442335)\n",
      "variable([[1.12672345]]) variable([2.26959351]) variable(23.97380754378544)\n",
      "variable([[1.48734571]]) variable([3.00386712]) variable(13.609686745040522)\n",
      "variable([[1.75641886]]) variable([3.557186]) variable(7.747049961219976)\n",
      "variable([[1.95666851]]) variable([3.97439789]) variable(4.43057410592155)\n",
      "variable([[2.10518573]]) variable([4.28923203]) variable(2.554280381353593)\n",
      "variable([[2.21482401]]) variable([4.52705574]) variable(1.492599869047195)\n",
      "variable([[2.29524981]]) variable([4.70694745]) variable(0.8916952181756939)\n",
      "variable([[2.35373273]]) variable([4.84325585]) variable(0.5514270962227455)\n",
      "variable([[2.39573972]]) variable([4.9467725]) variable(0.3585915308319281)\n",
      "variable([[2.425382]]) variable([5.02561369]) variable(0.24915731977561134)\n",
      "variable([[2.44575118]]) variable([5.08588371]) variable(0.1869065876539789)\n",
      "variable([[2.45917205]]) variable([5.13217364]) variable(0.1513533629631488)\n",
      "variable([[2.4673927]]) variable([5.16793652]) variable(0.13091003006317087)\n",
      "variable([[2.47172747]]) variable([5.19576949]) variable(0.11902210735018467)\n",
      "variable([[2.47316455]]) variable([5.21762597]) variable(0.11198198322254362)\n",
      "variable([[2.47244676]]) variable([5.23497527]) variable(0.10769231158094321)\n",
      "variable([[2.47013247]]) variable([5.24892259]) variable(0.10496655795675108)\n",
      "variable([[2.46664127]]) variable([5.26029927]) variable(0.10313337115761934)\n",
      "variable([[2.46228843]]) variable([5.26973075]) variable(0.10181280604960243)\n",
      "variable([[2.45731071]]) variable([5.27768752]) variable(0.10078974954301652)\n",
      "variable([[2.4518859]]) variable([5.28452363]) variable(0.09994232708821599)\n",
      "variable([[2.44614738]]) variable([5.29050548]) variable(0.09920140749444821)\n",
      "variable([[2.44019517]]) variable([5.29583359]) variable(0.09852769772358984)\n",
      "variable([[2.4341042]]) variable([5.30065891]) variable(0.09789878700703991)\n",
      "variable([[2.4279305]]) variable([5.30509512]) variable(0.09730181854646197)\n",
      "variable([[2.42171596]]) variable([5.30922787]) variable(0.0967293443169877)\n",
      "variable([[2.41549177]]) variable([5.3131217]) variable(0.09617698031441604)\n",
      "variable([[2.40928112]]) variable([5.31682532]) variable(0.09564208018092028)\n",
      "variable([[2.40310116]]) variable([5.32037549]) variable(0.09512298485383605)\n",
      "variable([[2.39696452]]) variable([5.3238]) variable(0.09461859803040694)\n",
      "variable([[2.39088043]]) variable([5.32711987]) variable(0.09412814592514404)\n",
      "variable([[2.38485555]]) variable([5.33035108]) variable(0.09365104127065577)\n",
      "variable([[2.37889464]]) variable([5.33350575]) variable(0.09318680628411545)\n",
      "variable([[2.37300101]]) variable([5.33659314]) variable(0.09273502898904006)\n",
      "variable([[2.3671769]]) variable([5.33962035]) variable(0.09229533840647547)\n",
      "variable([[2.36142374]]) variable([5.34259283]) variable(0.09186739042193233)\n",
      "variable([[2.35574235]]) variable([5.34551483]) variable(0.09145085969346782)\n",
      "variable([[2.35013309]]) variable([5.34838966]) variable(0.09104543497939387)\n",
      "variable([[2.34459602]]) variable([5.35121993]) variable(0.09065081640275062)\n",
      "variable([[2.33913091]]) variable([5.35400772]) variable(0.0902667138137311)\n",
      "variable([[2.33373736]]) variable([5.35675472]) variable(0.08989284577554084)\n",
      "variable([[2.32841486]]) variable([5.35946234]) variable(0.0895289389052284)\n",
      "variable([[2.32316275]]) variable([5.36213172]) variable(0.08917472741757472)\n",
      "variable([[2.31798034]]) variable([5.36476385]) variable(0.08882995278605695)\n",
      "variable([[2.31286689]]) variable([5.3673596]) variable(0.08849436347219049)\n",
      "variable([[2.30782159]]) variable([5.36991973]) variable(0.08816771469564999)\n",
      "variable([[2.30284363]]) variable([5.3724449]) variable(0.08784976822950144)\n",
      "variable([[2.29793221]]) variable([5.37493575]) variable(0.08754029221162851)\n",
      "variable([[2.29308646]]) variable([5.37739285]) variable(0.08723906096725743)\n",
      "variable([[2.28830557]]) variable([5.37981674]) variable(0.08694585483964615)\n",
      "variable([[2.2835887]]) variable([5.38220792]) variable(0.0866604600272269)\n",
      "variable([[2.278935]]) variable([5.38456689]) variable(0.08638266842618712)\n",
      "variable([[2.27434366]]) variable([5.38689411]) variable(0.0861122774778659)\n",
      "variable([[2.26981384]]) variable([5.38919004]) variable(0.08584909002056745)\n",
      "variable([[2.26534474]]) variable([5.39145512]) variable(0.08559291414552149)\n",
      "variable([[2.26093555]]) variable([5.39368978]) variable(0.08534356305679344)\n",
      "variable([[2.25658547]]) variable([5.39589443]) variable(0.08510085493499035)\n",
      "variable([[2.25229371]]) variable([5.39806949]) variable(0.08486461280463413)\n",
      "variable([[2.2480595]]) variable([5.40021536]) variable(0.08463466440508784)\n",
      "variable([[2.24388206]]) variable([5.40233244]) variable(0.08441084206493275)\n",
      "variable([[2.23976064]]) variable([5.40442111]) variable(0.08419298257969864)\n",
      "variable([[2.23569448]]) variable([5.40648177]) variable(0.08398092709285435)\n",
      "variable([[2.23168285]]) variable([5.40851479]) variable(0.08377452097997208)\n",
      "variable([[2.22772501]]) variable([5.41052054]) variable(0.08357361373597812)\n",
      "variable([[2.22382025]]) variable([5.41249938]) variable(0.08337805886540826)\n",
      "variable([[2.21996785]]) variable([5.41445169]) variable(0.08318771377558704)\n",
      "variable([[2.21616712]]) variable([5.41637782]) variable(0.08300243967265336)\n",
      "variable([[2.21241735]]) variable([5.41827811]) variable(0.08282210146035615)\n",
      "variable([[2.20871786]]) variable([5.42015291]) variable(0.08264656764154595)\n",
      "variable([[2.20506799]]) variable([5.42200258]) variable(0.08247571022229121)\n",
      "variable([[2.20146706]]) variable([5.42382744]) variable(0.08230940461854906)\n",
      "variable([[2.19791443]]) variable([5.42562782]) variable(0.08214752956532231)\n",
      "variable([[2.19440943]]) variable([5.42740407]) variable(0.08198996702823673)\n",
      "variable([[2.19095144]]) variable([5.42915649]) variable(0.08183660211747391)\n",
      "variable([[2.18753983]]) variable([5.43088541]) variable(0.08168732300399718)\n",
      "variable([[2.18417396]]) variable([5.43259114]) variable(0.08154202083800904)\n",
      "variable([[2.18085323]]) variable([5.434274]) variable(0.08140058966958151)\n",
      "variable([[2.17757703]]) variable([5.4359343]) variable(0.08126292637140048)\n",
      "variable([[2.17434477]]) variable([5.43757232]) variable(0.0811289305635685)\n",
      "variable([[2.17115586]]) variable([5.43918838]) variable(0.08099850454041051)\n",
      "variable([[2.16800971]]) variable([5.44078277]) variable(0.0808715531992303)\n",
      "variable([[2.16490575]]) variable([5.44235578]) variable(0.08074798397096412)\n",
      "variable([[2.16184341]]) variable([5.44390769]) variable(0.08062770675268231)\n",
      "variable([[2.15882214]]) variable([5.44543879]) variable(0.08051063384188899)\n",
      "variable([[2.15584139]]) variable([5.44694936]) variable(0.08039667987257197)\n",
      "variable([[2.15290062]]) variable([5.44843967]) variable(0.08028576175295649)\n",
      "variable([[2.14999928]]) variable([5.44990999]) variable(0.08017779860491725)\n",
      "variable([[2.14713684]]) variable([5.4513606]) variable(0.08007271170500452)\n",
      "variable([[2.1443128]]) variable([5.45279175]) variable(0.07997042442704135)\n",
      "variable([[2.14152662]]) variable([5.45420371]) variable(0.07987086218625004)\n",
      "variable([[2.13877781]]) variable([5.45559674]) variable(0.07977395238486742)\n",
      "variable([[2.13606587]]) variable([5.45697108]) variable(0.07967962435920853)\n",
      "variable([[2.13339029]]) variable([5.458327]) variable(0.07958780932814088)\n",
      "variable([[2.13075059]]) variable([5.45966473]) variable(0.07949844034293135)\n",
      "variable([[2.12814629]]) variable([5.46098452]) variable(0.07941145223842926)\n",
      "variable([[2.12557692]]) variable([5.46228661]) variable(0.07932678158554966)\n",
      "variable([[2.123042]]) variable([5.46357124]) variable(0.07924436664502324)\n",
      "variable([[2.12054108]]) variable([5.46483864]) variable(0.07916414732237737)\n",
      "variable([[2.11807369]]) variable([5.46608905]) variable(0.07908606512411756)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = mean_squared_error(y, y_pred)\n",
    "\n",
    "    W.cleargrad()\n",
    "    b.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W.data -= lr * W.grad.data\n",
    "    b.data -= lr * b.grad.data\n",
    "    print(W, b, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        diff = x0 - x1\n",
    "        y = (diff ** 2).sum() / len(diff)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x0, x1 = self.inputs\n",
    "        diff = x0 - x1\n",
    "        gx0 = gy * diff * (2. / len(diff))\n",
    "        gx1 = -gx0\n",
    "        return gx0, gx1\n",
    "    \n",
    "def mean_squared_error(x0, x1):\n",
    "    return MeanSquaredError()(x0, x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP43. 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_simple(x, W, b=None):\n",
    "    x = F.matmul(x, W)\n",
    "    if b is None:\n",
    "        return x\n",
    "    y = x + b\n",
    "    t.data = None # t의 데이터 삭제\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_simple(x):\n",
    "    x = as_variable(x)\n",
    "    y = 1 / (1 + exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8473695850105871)\n",
      "variable(0.2514286285183607)\n",
      "variable(0.24759485466749878)\n",
      "variable(0.23786120447054832)\n",
      "variable(0.21222231333102953)\n",
      "variable(0.16742181117834223)\n",
      "variable(0.0968193261999272)\n",
      "variable(0.07849528290602335)\n",
      "variable(0.07749729552991157)\n",
      "variable(0.07722132399559317)\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "# 데이터셋\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "# 가중치 초기화\n",
    "I, H, O = 1, 10, 1\n",
    "W1 = Variable(0.01 * np.random.randn(I, H))\n",
    "b1 = Variable(np.zeros(H))\n",
    "W2 = Variable(0.01 * np.random.randn(H, O))\n",
    "b2 = Variable(np.zeros(O))\n",
    "\n",
    "# 신경망 추론\n",
    "def predict(x):\n",
    "    y = F.linear(x, W1, b1)\n",
    "    y = F.sigmoid(y)\n",
    "    y = F.linear(y, W2, b2)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "# 신경망 학습\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    W1.cleargrad()\n",
    "    b1.cleargrad()\n",
    "    W2.cleargrad()\n",
    "    b2.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data -= lr * W1.grad.data\n",
    "    b1.data -= lr * b1.grad.data\n",
    "    W2.data -= lr * W2.grad.data\n",
    "    b2.data -= lr * b2.grad.data\n",
    "    if i % 1000 == 0:\n",
    "        print(loss) # 1000회마다 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP44. 매개변수를 모아두는 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Variable):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable, Parameter\n",
    "\n",
    "x = Variable(np.array(1.0))\n",
    "p = Parameter(np.array(2.0))\n",
    "y = x * p\n",
    "\n",
    "print(isinstance(p, Parameter))\n",
    "print(isinstance(x, Parameter))\n",
    "print(isinstance(y, Parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, (Parameter, Layer)):\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p1', 'p2'}\n",
      "-----------------\n",
      "p1 variable(1)\n",
      "p2 variable(2)\n"
     ]
    }
   ],
   "source": [
    "layer = Layer()\n",
    "\n",
    "layer.p1 = Parameter(np.array(1))\n",
    "layer.p2 = Parameter(np.array(2))\n",
    "layer.p3 = Variable(np.array(3))\n",
    "layer.p4 = 'test'\n",
    "\n",
    "print(layer._params)\n",
    "print('-----------------')\n",
    "\n",
    "for name in layer._params:\n",
    "    print(name, layer.__dict__[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, (Parameter, Layer)):\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __call__(self, *inputs):\n",
    "        outputs = self.forward(*inputs)\n",
    "        if not isinstance(outputs, tuple):\n",
    "            outputs = (outputs,)\n",
    "        self.inputs = [x.data for x in inputs]\n",
    "        self.outputs = [weakref.ref(y) for y in outputs]\n",
    "        return outputs if len(outputs) > 1 else outputs[0]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def params(self):\n",
    "        for name in self._params:\n",
    "            obj = self.__dict__[name]\n",
    "\n",
    "    def cleargrads(self):\n",
    "        for param in self.params():\n",
    "            param.cleargrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Layer):\n",
    "    def __init__(self, in_size, out_size, nobias=False, dtype=np.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        I, O = in_size, out_size\n",
    "        W_data = np.random.randn(I, O).astype(dtype) * np.sqrt(1 / I)\n",
    "        self.W = Parameter(W_data, name='W')\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(O, dtype=dtype), name='b')\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Layer):\n",
    "    def __init__(self, in_size, out_size, nobias=False, dtype=np.float32):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.W = Parameter(None, name='W')\n",
    "        if self.in_size is not None:\n",
    "            self._init_W()\n",
    "\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_size, dtype=dtype), name='b')\n",
    "\n",
    "    def _init_W(self):\n",
    "        I, O = self.in_size, self.out_size\n",
    "        W_data = np.random.randn(I, O).astype(self.dtype) * np.sqrt(1 / I)\n",
    "        self.W.data = W_data\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.W.data is None:\n",
    "            self.in_size = x.shape[1]\n",
    "            xp = cuda.get_array_module(x)\n",
    "            self._init_W(xp)\n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649353)\n",
      "variable(0.07888166506355147)\n",
      "variable(0.07655073683421637)\n",
      "variable(0.0763780308623822)\n",
      "variable(0.07618764131185572)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "# 데이터셋\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "l1 = L.Linear(10) # 출력 크기 지정\n",
    "l2 = L.Linear(1)\n",
    "\n",
    "def predict(x):\n",
    "    y = l1(x)\n",
    "    y = F.sigmoid(y)\n",
    "    y = l2(y)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    l1.cleargrads()\n",
    "    l2.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    for l in [l1, l2]:\n",
    "        for p in l.params():\n",
    "            p.data -= lr * p.grad.data\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP45. 계층을 모아두는 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, (Parameter, Layer)): # Layer도 추가\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def params(self):\n",
    "        for name in self._params:\n",
    "            obj = self.__dict__[name]\n",
    "\n",
    "            if isinstance(obj, Layer): # Layer에서 매개변수 꺼내기\n",
    "                yield from obj.params()\n",
    "            else:\n",
    "                yield obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(None)\n",
      "variable([0. 0. 0.])\n",
      "variable(None)\n",
      "variable([0. 0. 0. 0. 0.])\n"
     ]
    }
   ],
   "source": [
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "from dezero import Layer\n",
    "\n",
    "model = Layer()\n",
    "model.l1 = L.Linear(5) # 출력 크기 지정\n",
    "model.l2 = L.Linear(3)\n",
    "\n",
    "# 추론을 수행하는 함수\n",
    "def predict(model, x):\n",
    "    y = model.l1(x)\n",
    "    y = F.sigmoid(y)\n",
    "    y = model.l2(y)\n",
    "    return y\n",
    "\n",
    "# 모든 매개변수에 접근\n",
    "for p in model.params():\n",
    "    print(p)\n",
    "\n",
    "# 모든 매개변수의 기울기를 재설정\n",
    "model.cleargrads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(Layer):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAJ7CAYAAACmr6OdAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdeXxU5d3//9eZJSSBEAIBAsGwyqqogEvdFdDetihVcWOzLqB33e5bLXj/9O5ifwpVWq22KHjbBXesSq3UCkrrhgsgKJsgWwgBAoEsJGSbme8f15zMZCUJyZyZyfv5eMzjzJw5M+czk1xzzudcmxUIBAKIiIiIiIhIzHA5HYCIiIiIiIg0jxI5ERERERGRGKNETkREREREJMZ4nA4gbvkroeoIVJWCvxwqiyDgg0AVVBbX3b6qBPwVddd7OoIrodZKCxK6mLvuRHAngacTuLyQkNbqH0Uk7vjLg2WzIlj2guUVTDmtLKr/db6j4Cur/zlXB/Ak1/+cNwWs4M+tXVY9yeY17iRTjkWkLvuYaZe9ykII+ENltzb7+RrCjpnh7DJoucCbGirDdhkVEYlySuQaUlkMR3Oh/ACU50NFAVQchsqC0P2KQ1B+EMoPg+8IVJaYg459QugUyx06OXR3MAewhG7QId0keglpZp23S+h+QldIzoQO3XVSKdGpshDKDpgyV1lgymjFYbOsKg4tKwpM2awohKpCk7BVHTUnfvYtGrkSgrdgkudJBk9nSEg15debahJCT4pZejub8utNMc91SDflt74TVhGnBKqgLA/K9ptb9fHTvgXLa1nwWFtVGCqvVSWAU+OxWcELqcHy6O1symFid3O8rD52poVuHbpDUi9I7BG6cCMi0oasdjdqZVUplOyE0mwoyTbJWtl+KN1j7h/NNQeT2id7ltvcsIBAsHbN58AHOE6WJ/g5APzBz1Hr6qWnU/CAlAEdsyCxp7klnwAd+5p1SZm6YinHx3fUlLuyfcHytzd4YeQglOfB0f1mWX4oeJW9nvLm8gIWWBYEAoAf/FWR/iSRYXlMzQEWWAHzef2V9WznDjvpTDfluEOPUKKX1BOS+pj1SZkN1yKKHEvFYSjZFbztNGXYvpXuMslZRQE1kzELXB5CPTt8sVdmw4+jAb9JVmt/xoQuJulL7mPKW2JPSOodPIYGbx26ORG9iMSR+EvkAj5zQCnaAkXfmvslO+HINijZbU4IbZYHXO5gM416TogEc9ANniz7K4Fg0me5zIlhx76QMtgkdx37Qech0HmoOWhJ+1VVAkd2QMkOszwaTNRKsuFojknSatRch/2fofLY6lxeU2ark7+wn31PMiQGk7qOfU2NQnImdOwPnfqZpTfFqcjFSdXH083meHpkJxzZDke2QmmOuTBqc3kBl0lqYvEiZ5tyg9sTLH9hF4k9SSbR63QidBoQOp6mDoNO/VWrJyLHFLuJXFUJFK6Hwk3mAFO02Tw+sjN4dYzQiWGgMni1XtqGC9xec/C2r6x6OkLKidDlZJPcpQyB1OHQebAOTnEhYK7CF20JJms7oWS7eXxkp2n6aLNP8PxVgE7woprlMn+v2he3vKkmqUuxTzj7mxPNlBPNBRxL42bFNH9l2PF0kzmeFnwNxTvM8RNCfbWjtWlyrKv9/VoeU+ZSTzaJXeow6DzMHFPr9JsXkfYqNhK5isNQuAEOrYZDq+DASnPyGPCbEwjLHTxJjP6P0u5YbnNi6CsHAuZxpwGQfhZ0HQ2pIyDtNDUxiVb+CnPlvXADFG40NduH1pj7vqNmG7sM1tdMV+KL5QLLS43mcJbH1OKljYTUk0z5Th0OaaeaZtoSXaqOmIuf9jH1wCcmafNXqixHI8tlErdAJfh9wWNof0j/njl+pg6H9LN1DBVpp6IvkasqDSZrn8CBj+DgZyaRg+CPWZUOMPHA5QUCwZNBCzqeAD3ONwek7ueYA1R1Xz6JiKO5cPgrOPRV8KLJl2ZdIIBp+hg8mVD5k3oFa+b99v+IBcm9IW00dBttLtiknWr62krkFG8NHU/3rzBNJQMB008tQKgFi8QWy2P6BvsrMcfQvtDzfOh+vjmGdh7qdIQiEgHOJ3LlB83B5cAnkPehuTIY8IE7AXxVVPfJkvhX3XncZ/rtdDsTelwA3c81ByaNptl6SnPg4EqTuOWvMolbxSHznDsxeDKuZpDSGoL9H+0mY97OkDYK0s8wyV3698xJqLSCABxeC/v/DQc+NMuKQ8HBulzqexrvwo+h3lTocR70uBB6XmDKnJpAi8SdyCdyAZ850OxbDnveMQkcwauDOshIDZaZPsFXZmqDup8DvS6FjHHQdZR5Xo4t4DN9Xuyr8gc+NQMW2KPHqVmyOMGdCL4KwG8GTko/E7qfZ8p5tzPUD6ipyvNh/wewbxnkLDFD/VtuzOjKuhDarlluU3PnLzeJXa/xkDEeMn9oRtAUkZgXmUSuPB9y3oLdb5jaN9/R4Al6eZvvWuKJFey/UWVO/Pr8EE642hyYdNIXJnhVPvdd2P++aZ5cVVKrKY5IlLGndwlUmSSv2xmmbPf+vmoTajuyDXa9AtlvQMFacx3G5dFAJNI4O8EnYAZROeFK6HudGZBMRGJS2yVyFYdg91uw6yXY/6/QejXXktbiCvbH8aRA1pWQdY2prWuPSV15vrkiv/cfkPP3YHMqbz3zG4nEirAmmd4ukHkZ9L4Mel1i5sNrb0pzIPs12LHIXKixf/9EWsr+H0odAf2nQt9rzSi0IhIzWjeRC/hMc8mtfzBNJ8PXi7Sl8KSu3/Vw4u1mYIV4VpoDu16FXS+bvm4AuEPDhYvEE8uD6TMdgNSRppz3vS6++9f5K0xLli1Pw8FPw0aU1MUZaU1hzey7joEhd5ikztXB6cBE5BhaJ5Er2wfb/g+2/N7ct9TfTRxkeUxNVNcxMOQuyJoUPwOllB+A7Ndh5wtmsBLLo6HCpX1yecxw7F3HwICppkY+safTUbWOo3vhu2fNMbX8kGkSrQuiEgl2E2ZPCgz+CZx4m0aaFYlix5fIFW+Fr38O2a8G+95oGGOJJi4zHoonBYb+Fwz9b/CmOB1UCwRg73uw+bewd7n5TIEAGtFVBExtgttczOh5kSnrvf4jNvvUHdkBXz9k+r9plElxmstrLpZkXQWn/ApSBjsdkYjU0rJEriQb1v8Stv8J05RLHawlyrk84O4IJ/1/cOJPzPQG0a6q1NS8bZ4HRVvUJ0bkWOymhx37wbD7YMD02JiUvCwP1v8Kts7XgEQSfeyEbtDNcPLPNeKlSBRpXiLnOwpf/y9sfjJ4sFECJzHGckNCGoz6jencHY2qSmHz47D5N1B5BA0jLtJclqnRcifCkHtg+KzorI0P+GDTY/DNL01zcCVwEs1cXlOuhs+CEQ+axyLiqKYncvmfwyeToXSXmlBK7LMs6P1DOPM5SOzhdDRBAdj5Mqy5F8oPBkecFJHjYnkgoQuc9mvoPz16mlwe2Q6f3gD5q9T/TWKL5YYuJ8M5r2jqAhGHHTuRC/jg65/BhjngUj84iSOWxzS7OvsvkDnB2VgOr4MvboVDqyBgof5vIq3Ngi4j4aznzAApTtqxCL68zdTAqRZOYpHLYxK60U/BoFudjkak3Wo8kfMdhY+vhdylumIoccoFVgBG/RaG3O1MCFvnw+q7gYAulIi0JcsDBEzt3ND/diaG9Q+bi6Oa31HixfDZcOqjTkch0i41nMgFquDfV5jR8tTES9qDMb+DwXdGcIcB04zy2yc0L5RIRLlg0C1w+h9MrUKkbHoc1v5U5V3ijAtOfsgMhCIiEdVwZ4G1D8Def8ZtEpdXBPOWOh1F/eYthcLS5r/us+/g9ufBmgxXPQEPvAqXz2v9+OLW6rth37LI7W/tbPj2SZ3UhYnmctlaVL6jgd/Mfboqghdu9i2Dr5xP4vKK4JWVbfO/Ey3lV2Us0vxmwJ6ct5wORKTdqT+RO7jSDHkep80p84rgZ6/DhNNC69Zlmx9v+3b78y1778JSczBYuKLxA8Dba8zzl88z98ONOwmmzjdxNtUHG+B7P4MHroDAi+bxnL/Vfe/WUFhqvqOGrMsOff7Gtlu4ovHnI8+CT6eYUSPb2u43YOOvo3I0yvByYE0OnZjNW1r3uez8hl/XXLXLZWuUpeZozf3FW/nOK4KHFof+tq+srPu67PzQSfDtz5sYo1rAB989Azv+0vb78pXBp9PMIEsO+9nrcP3Trf+/05blt7llXGXMCRZ8djNUFjsdiEi7Un8i9/VDQASbm0RQYSncshCmnw+De4XWf7Gt5naXndqy93/8HXjnK5jxXMMHgFdWmoPbotvNbela89h2Shb8zxUmzqZeVVz8uVlmdTPLwwtbFn9TfLi54efmLTUHo4xUePpGc0Csz7ps8x1FlYAfyg/Btjb88sBM2/HlT2isQtxJgRdh5S/M/ccnw72Xmfv3Xga7fge3jQ1tZ/+/2Y8X3GJe09DfvSH1lcvWKEvN0Vr7i7fynVcE2/Pg4Unm7/ryHSYRCK95KSyFdbtg/k1QsBAuGAZjH2mbk+BWFQBW3QNVR9p2P9v/BOUHouLCzfybWv8927L8tqSMq4w5wQ+VRebiiIhETN0zyfJ82L8ibptUPvcv8yN/1qCa6zNSzQ+ofZswqmXv//Akc2tIdr75gf6fKyA12dxuG2cOduuyQ9udNQgy00y8TfHM+y2Lt7kKSxs+iN7+PBSUmIPthFE1T/Jrv8frn7ddjMcl4IMdL7TtPnKXQvl+onlkyrMGmYTt35tqrs/qZq5YA2zZW/O5wlLzfzzpzObvr75y2VplqalaY3/xWL6359X8u1z3PbO8LyxZ/3Bz6DczNTm0TfQ3SwtAVSHsfrNtd7PzZaK5vB+vtiq/x1PGVcYcEKiCnc28iicix6VuIlfwdVRcNbTV11yrvnUPLTa3xuQVmR/Gi4bXXJ+db34MH1psmoK0pU+3mGXvtNC6Xl3Msnat4KQzTbyNNQ+p3YztWM3aCkvNFU57u4Ur6r6/faCxt3locWibx98JXQGs/f2DOXCnJje8fzAH1jsvbXwb5wRMGWjLEeUOfgZWQtu9fyuZfr75W9e+4pt72CzX7Ky5flOuSf7CE/jjKZfH0pyy1Bqasr94LN+1L3rZNRwPTgyta+jCl117G93cpjtBWzr8leN94+pj92mzm+qFN5UG58vv8ZZxlTEHFGxAI7KKRE7dRC7K2jfbzbUA9s8PLSeMgrWPNq8J1+fBJG1QRs3163aZ5a/eMm3kL5/XvLb1zWHXcISf7PbobJa1T5jtOD9vJLm0axAbelzb1PlQXGa22T/f7LN285PZr5grnvvnm6Z0v3rL9H2AmldY7X2tyzbbXHZq6OB1+bz62+9/sAHOGRz6zFHJXwG+8rZ7/4rDQPT3P7VPLmpfPX7nK3Py8NKndddPP7/5+2moXB5Lc8pSa2jK/uKxfIfLzjcnogBTz204Dnt/LW2iHlGBKqg41Lb78EWg320LbM8zTab3z4c9h6HvXc0/9rVl+T3eMq4y5oBAFVQddToKkXajbiKX1KuezZx160XmxLHn7eYgs+hjeO5W05TDdqxmHBC6gle7yd+EUabN+dpHzRWwt9fAktWt+xlsjTXfqH1gSk0yyy37WmffH2ww+7hitHnco7NpsvL2GvjHutB26Snm++7ROfRdNRb38vVmmZVu/lYFC02TlrGP1KzhzCuCbXl1rz5GHW9ncCe23fsnnwBWdPaPq23BLeb/w25GWVhqEvcHgv83dvOmwlJzsjKsd83XH0+5PJbmlKXW0JT9xWP5tmXnmxP9XwUHpnv7q4a3Xb3D/K6eP7RlnyWiXF7o2Ldt99EhvW3fv4Xs3+IeneHxG8z98GOf0+X3eMu4ypgDPJ3Ac4xmOSLSauqeTaadCt5ODoTSuF9cbZa3LDSjYrWkRudXjYyMm5psEsOHJ4VOXp1mN1G8r5WanNuducO/O/vEO7x25eFJpkN1dn7ThpK247MTa7sfA8CfPwxtt2S1SfSimssDGePadh+ZPwRfRdvuo5VcEDxJsJtRbso1f8OsbuYkwj6J25Rrys2xmtXWp7FyGc9ipXzbsrqZ2gP7gtd9LzbcX/aJd0P9mqKevwJ6/7Bt95Exzvy2RDF7kJLmDkIVzeVXZSzCLDf0vNDpKETalbqJnMsLA28xyyjSo7MZyentNXCopG33dc2ZbZfINTaISlu3da/viqB9EKj9eReugDv+VHOKhuawkzp7n2+vgUtHtuy9IspfBYNmtu0+0k6FjLFRV8bqM7iX+b+8/mnz+J2v4MzgVfwbzg6d9L3zFZwxMLKxRbosNWV/7aF8n5IVavJV30n/KyvN9xD1Ne9gkqtuZ0CPRtqwtYaBt5jfFqkW7eWpPipjxxDwtf3xU0RqqL991/AHTPV4FDX/yisybfgfn2z6sbVVHzYI1ii10UHCPjCFx293MB/Vv2322di+beGf95WV5uDx9I01p2hoiP3a+oZ5tvd5+TzTZKS+ecaiZi45l9dcOe91Sdvv64xnTfPNKCpjDbFHobT7PNpXo0f1C61fl12zqXMkRLosNWV/8Vi+69PQ69Zlw4acGKh5B8AyZf6s5839ttTzQuh1aUxcvIlUghTt5ak+KmONcHmh+zmQ+QOnIxFpV+o/i0zsAWcvinAojVv0semUfcuF5sfU7jjcHI8HE4ZjzStTWNqyIdSbwq6V2p4XWmePAthQjVX4yFXH44az6+7b/i7CP69d+9LUPg/2a3cerPu+9j7Dp3aor3O541we8HaB70VgcmCATgPh/LfA8pjmKFFsdPCE6Yl34QdhV5ftE42xjxzfSUVTy2VtLSlLx6Mp+4vH8l0f+31fviO0Lq/I9JcN71O1LtuMhhh1LJcpd+cuhtQRkdnnWX8ET2dT5qOQ3d/1gmHNe11blt/WKuMqY23McoM7Cc5+gTa/KCIiNTRcHdD7B3D6MzhdKAtLzdC9t1xoHqcmm3nKnnm/5rDITRkmeXBwBKvCsAGVXllZc3TF7HwzX8vFtY7t9hDNTZmfKvyAVvvgltXN9CX684fmucJSc3/BLXV/9O0rj8dqshYekz0oRfgVQ/v+f5xikuBHloTW/WOduZIY/nntq47Z+TXnCrNfE35Vct5S89oHJ9YcYvm1z8129lw3Uc3ygKcjjP0gsoP99LwYLl5uOoZb0XulPjXZnKy9vaZuUx57RNkzG2ji09JyaWuNshTJshuP5fvyeWZpx1tYakbVe3BiqHznFZn+y/e9WLPW/dQHonBUPctjasMvWmqOc5GS1Asufg/cCY4nc/bf2D725RWZcvr45Jq/2U6X3+Mt4ypjEWC5TW3cRf+Ajv0cDESkfbICgWNMbrP7r/DJ9WYOHAcmCQ9vdmfX3NRuihd4seY8Zg3JKzIjX678ReiE9O01oQk1H5wIV59ZfxOxhxbDwWLTvPNv9zYt3toxhrP3O2EU3PP9uokjmBEfv/czM4RxQ4O7NLVZor3/vCIz6Ijd7v7lO8zBKbzD9Lpsc3B4cKKZ7+2pf5rP/sAV5uBZ+3k7toUrQu+74BbT17Cxjth27I7WyLm80KEHjF0OnR0a/qskGz66Gg6vjqo5HMOtyzYDm9SueVuXbSZ3b6jctbRcQuuVJSfKbjyV78+/qznp8OOTzRQi4X+r259veFS+bx9vefOyVmd5TDk//w1IOdGZGAq+gQ/GmSkPHOw398EGU8v+9hqTiEw6s/6yA86W36Zs01AZVxlrYy6vuShy4T9Ms0oRibhjJ3JgDjyfXA/F38Z8h217BKl7L2vZ6y+f1/jJYGt6aDF06djyWKUxlqls7nOl6a/W4TjawLSGQBVsex6+mg2+4pgvZ811vOWyKSJZdptC5TuCLC+43DB8Fox4AFwdnI2nbD+snA57lwHRefGmOSJRfpuidhlXGWtLLug2Gs552XQVEBFHNG2khS4nw3+shqH/bfoWRPkwyo255UIzyWj4/GZN9dl3ZrjfSFiXbW52k1JpRS6vmSvu7JfhvNedT+LA1BQMmgGXb4WBtwKumBgYobUcT7lsikiW3aZQ+Y4QlxcsCwZMhSt2wsk/dz6JA0jsaZqijf5NMMbYLuttXX6bonYZVxlrI5bXHK9OfQQu+UxJnIjDmlYjFy7/c/jqp5D3oSnMDjS3PF52W/OHJzV9pL0PNkCfrpFpvrBlL/z2H2buvJbMlycNsLyAHwbebE7oItkfrrkKN8Cmx2DHS6bm0F/pdERtriXlsikiWXabQuU7Alxe0x0g62pTC5cWbR31whRtgjX3Qe5Sc5E0Rmvj26r8NkXtMq4y1gYst5leIGMsjPoNdImF+YRE4l/zEznbvvdh7Sw4tDpUwGNIYSk896/obHIxb6mZQ0YHoFZieQEf9L0BRv4COg1wOqKmK8uD756Bzb+DysOAFXNlrTmiuVy2FpXvNmK5gYAZGXLIHXDi7ZDU2+momu7AJ7D2fjiwMiaPqRA95VdlrBW53OD3QdooGPU49Iy2eQ9E2reWJ3K23Hdg829g3wpzFdRf0UqhiRwPK9QMuP8UGHovdG7muNrRxF8B2a/B9r/A/g8wCZ2feOhfI9JiloXpIRCA7ufBgB9D32vNAAyxKvcfpjZ+/790TBXnuLymJUj62TD8p9DncpwexVxE6jr+RM5WvBW+exa2LgTfEeK95kCilOWFQCV0GgRD7oQB08Gb6nRUrav8AGS/Djv+Ypo6W27wK6mT9sIKNevvOsb0f8uaBIkZTgfWuoo2w5anYfvz4KsAAlE7qq3ECcsyxxPLA/2nwuA7zRgJIhK1Wi+Rs/nKIHsx7HwJ9i03/RRcVsy2+5cYYPcr8aaYPjH9pkHPC2gXVw9Ld8PuN2DP3yHvI/CXg7sD+Mqdjkyk9di1A5bXDHOe+UPIuqp9zFtVWWSOpzv/Agc/MyfZfh+6cCOtwk7e/D7oOtokcP2nQUIXpyMTkSZo/UQuXEUB7Pkb7HrFDLMc8AXb/iupk+NkJyveFOhzBWRdA70uBVeC05E5x3fU9LPZt9wkdoUbTPNSXCpzElvswUoCVZDcB3pfBhnjTBn3tuOOT6V7YPfrsPNFyF8VLN+qqZPmCnY78FeY+RT73QD9pkDKoGO/VESiStsmcuEqDpu2//v+CXuWQvnB4MHap4OQHJs9kEHADymDoc8Ec1LX48J2NUx/sxzZAXn/hgMfm/42R7abSkpXgmrsJLpY3uDFhgAkZ0HGRabPW48LdHLZkCPbzAWbPe+YUaT95aZsq0+d1Ke+Wu0+E8zxVERiVuQSudoKvoa975khlw98Yg4+9uiCSuzE5TX/BwEfeLtA7++bxK3XJbE1El00KT8IBz41iV3ev+HQV6Y/oeUONq3RCaBEgCsheAEv2EIj7VSTsHU/15xgJvZwOsLY4ztqynTuPyDnb1CyE7A0WEp75koITlsTgOQTzGAlvb5vRp30dHQ6OhFpJc4lcuH8FWYag4MrIe9jOPCROem0XMH+ADoQxbfwEw4LUgaYmrbu50D6WdB5KO2iv1uk+SuhcCMc/src8r+EgnVQVYr5mySYRE8XVqRFLHAnhAbqcCeZgRO6nWGSt7TToMtJ7bs5dFs5mmsukB74GPavgIKNgC/49wie3EscscDtNWXNcpljZs+LzTG0+7mmebKIxKXoSOTqU7LTHIjyvzBJ3uG1UFViOua6Ek0zEp1gxh7LjemzFZzgOrGH6WDddYxJ2tK/BwlpjobYvgWgeFswuVtrRs4r+BqO7Az1s3N3MLUpGsBIANPfxh2atN5ymxqAtJGQOgK6nAJdTzMjyVouZ0Ntr6qOmIFSDnwCh1aZ/nVl+8xz7gQz0IVGmY4Nlgtwhx1Du5vjZ9fTofvZ5hjanvuRirQz0ZvI1efIDnNyWbAODq0xCd7RvZiri3YNQpUOSNHA8nK00iLJE6xNtTyQMhC6nQ5dTg1ekT8VOnRzNk5pmkCV6WNXuMkkd0XfmrJY/B1UFYe2s2tXVIseX+r7u3o6mf5rXUZC6jBIGWKWnQaq32osKD9oLtgc+goOrzFTmZTshoCP7QcsUpK9dO+kBM8x9jQA/mCNtuUyNWvdzoSuo0yNdtppaoos0s7FViJXH99RKNoCxcFb4WYoXB88wTxitrGsYP87dILZWuzJtgOB0JV4MFcHU4bi7zycQVcvZmD/vsy49SYmXnsL3oQYnqRXGlZZaGrsSnaElsXb4MhWc2LoOxra1nIH/2/8Nf9vxDkuL9Ujm4aftLsTzYljymCTnHXqBx37h5YanjyuVFZW8vbf3uTZPzzJ8n+t5P+/6xJmX9PD1MgXbw02uSb42+9VGW4Nltd8n+FN2D1JpvY6/AJJ5yHQeTC4Ojgbr4hEndhP5BpTlmeaaJbsgpJsszyyHY58B6U5pqlmtWA/LSs45117vgppecHlqv9A3aEbdOxrTuw69jP3O/Y19zv1M1fpAZ/Px5tvvsnChQtZvnw5PXv25Mc//jG33HIL/fv3j/QnEieVHzR9dkpz4Og+OBpclu4O3vZAxaG6TaVd9klOINiMU02pm8TuW4wF1FOGLQsSuppBg5KzTLKW1Cu4zICk4GNd6W8X9uzZwwsvvMAf/vAHcnJyuPjii5kxYwYTJ07E6w2rWS3bZ1rFlOwMXrDZZUbOLN4GR/fUvEhq1yYRCCYp8XuaUb9GzicsLyT3Ni1UahxH+0Gn/hrMS0SaJb4TuWOpLDQnkeUHzIlmWR6U7TfNNe0TzrL9ZkLWeodrt0ztAna/j4C5BaqcPXBZ7mA7eovQICGN9GnydDT90hIzgid3meYkLrGHedyhuznBS+7ToiuCOTk5vPjii8c+UZD2K+AzZa08P3g7YBLA8vywZbB8lh+AiqLghZgGypk9EqdlBVteB4LzkgWid049l518BW8WwZFb/Y1fWPJ0MvMpJnaHDj0gsSd0SDcXXTp0C97vHrqf2DPYV1XaK7/fzwcffMCCBQt48803SU9PZ/r06cycObPlF9qO7g0eP4PH0qO55nHZfpP0Hd1vynJ4U+xwLk/o/zJgH0udHMU62PezxnG0kVpITydIDJav5KzQxZCk3sHjaa/ghZJeYe8nInJ82nci1xz+cjPBecVhs6wMu19xOJjsHfBkFkQAACAASURBVAVfGVQWm6uQZQfMj35VkWmW4g8mg76yehJDH1QdrbNbu4arBm+n0AHP08lc+UvoZvqxJHQxo8O5E83JnSvBJGneLua5hLTgrUtofYQGIPD5fKxYsaL65KFbt27ceOON3HrrrQwcODAiMUicqTpiyl5lsTlBrCgIPi4yjyuLzf1AVbBc+oKP/eakEqAiP7gsoDox9FfVqrGvtU9gfQ74/HBKVnC9pyP1nqB5kmv2GbMH80kI9g/t0NXUXnhTzNIu094U8KSYpbczeFODt+Bj+zmRJsrNzWXRokXMnz+f3bt3O3dRreJwrVtBzcf+clP+qkpMTV/ZweC6WsfSqqN1u0v4K+qus7xmkKZwLq8pmwBWAng7mjLlSjCJl8tryqIn2VzArD521r4Fj6VKzkTEAUrkxBF2c5758+eza9cuRo8ezYwZM5g6dSpJSUlOhydyTFOnTqWoqIglS5Y4HYpIg2rXvtkX0GbMmMGAAQOcDk9ERI6DxoIWR2RmZjJr1iy2b9/OsmXLGDBgAD/5yU/IzMxk5syZrF+/3ukQRURi1t69e5k7dy4DBw5k/Pjx5Obm8tJLL7F7927mzJmjJE5EJA4okRNHuVwuxo0bx2uvvcauXbuYNWsW7733HieffDJjxoxhwYIFlJaWOh2miEjU8/v9LF++nGuuuYa+ffsyd+5cLrnkEtavX8/HH3/MpEmT1C9ZRCSOKJGTqNG7d29mzZrFtm3bqmvp7rjjjupaum+++cbpEEVEos6+ffuYO3cugwYNYvz48Wzfvp2nn36aPXv28OyzzzJixAinQxQRkTagRE6iTu1autmzZ7N8+XJGjhypWjoREWrWvmVlZTFnzhzGjx/PN998w6pVq5gxY4b6G4uIxDklchLVevXqxaxZs9i6dWuNWrrevXszc+ZM1q1b53SIIiIRs3//fubOncvgwYPrrX076aSTnA5RREQiRImcxITwWrrs7GweeOAB3n//fU499dTqWrqSkgaGixcRiXGrV69m2rRpZGVl8eijjzJ27FjWrVtXXfuWnJzsdIgiIhJhSuQk5mRkZDBr1iy2bNlSXUt35513VtfSrV271ukQRUSOW0FBAQsWLOCkk05izJgxbNy4kaeeeorc3FyeffZZRo4c6XSIIiLiICVyErPCa+n27dvHY489xieffMJpp51WXUt35MgRp8MUEWmW1atXM3PmTHr37s3999/POeecw9q1a1X7JiIiNSiRk7iQlpbGjBkzWL9+PatWrWL06NHcc8891SNerlmzxukQRUQaZNe+2VOvrF69mieeeKK69u2UU05xOkQREYkySuQk7owePZpnn32W3NxcHnvsMVauXMno0aOra+mKi4udDlFEBAjVvmVmZnL//fdz9tln89VXX1XXvnXs2NHpEEVEJEopkZO41aVLF2bMmMHXX39dXUv3X//1X2RmZjJt2jSWL1/udIgi0g4VFhayYMECTjnllOrat9/+9rfVI0+eeuqpTocoIiIxQImctAt2Ld2ePXt4/PHHWbduHePHj2fEiBHMnTuXQ4cOOR2iiMS58L5v9913H2eddRarV6+urn3r1KmT0yGKiEgMUSIn7YpdS2cP233uuefy8MMPk5mZyTXXXKNaOhFpVUVFRSxYsKB6qpSPP/6YRx55pLr2bdSoUU6HKCIiMUqJnLRb4bV0Tz75JFu3bmX8+PEMHz6cuXPnkp+f73SIIhKjwmvf7r77bgYPHsyyZcvYsGEDd999NykpKU6HKCIiMU6JnLR7qampzJgxo3qAgfPOO4+HH36YPn36VNfSBQIBp8MUkShn176NGjWKMWPG8NFHH/HQQw+xZ88eXnvtNcaNG+d0iCIiEkeUyImECR/x8sknn+S7776rUUt38OBBp0MUkSgTPvLk3XffzaBBg1i2bBkbN25k1qxZdO3a1ekQRUQkDimRE6lH586dmTFjBmvWrGHVqlWcf/75/OpXv1ItnYgAUFxczIIFC6qnNvnwww958MEHycnJUe2biIhEhBI5kWMIr6X73e9+x7Zt2xg/fjzDhg1j7ty5HDhwwOkQRSRCNm7cyOzZs+nbty933XUXAwcOrFH71q1bN6dDFBGRdkKJnEgTpaSkMGPGjOrhwi+99FLmzp3LCSecoFo6kThWVlbG4sWLq6cseeutt5g1a1aN2jfLspwOU0RE2hklciItMHr0aJ588klyc3NZtGgRhw8fZvz48QwZMoS5c+eSl5fndIgicpw2bdrE7NmzyczMZOrUqaSlpbFs2TI2bdrErFmzSE9PdzpEERFpx5TIiRyHxMREJk2aVD2s+JVXXsmvf/1r1dKJxKjw2rfhw4fzxhtv8NOf/pTdu3er9k1ERKKKEjmRVjJ8+HDmzJnDnj17eOGFF6pr6QYPHszcuXPZv3+/0yGKSAM2b97M7Nmz6dOnD1OmTKmuffv222+ZNWsW3bt3dzpEERGRGpTIibSy8Fq6jRs3ctVVV/HYY4+RlZXFhAkTWLx4MT6fz+kwRdq98vLyGrVvf/3rX7n//vtV+yYiIjFBiZxIGxo2bFiNWrqysjKuvfZa+vXrx+zZs9m9e7fTIYq0O1u2bKlT+/bee++xZcsWZs2aRY8ePZwOUURE5JiUyIlEQIcOHapr6TZt2sTkyZP5v//7P/r378/48eNZvHgxVVVVTocpErfCa9+GDh3Kiy++yM0338y2bdtU+yYiIjFJiZxIhA0ZMoQ5c+aQk5PDyy+/DFCjlm7Xrl0ORygSP7Zu3crs2bM54YQTuP766wF49dVX2blzJ3PmzKFPnz4ORygiItIySuREHBJeS7d582amTJnC888/z4ABA1RLJ3IcKioqqmvfhgwZwgsvvMBNN93E9u3bWbZsGZMmTcLtdjsdpoiIyHFRIicSBQYPHsycOXPYvXs3r7zyCmBq6fr27cvs2bPZuXOnswGKxIDvvvuuuvbtuuuuA2rWvmVlZTkcoYiISOtRIicSRcJr6b799lumTp3KH//4RwYOHKhaOpF6+Hw+3n777eqpPhYtWsSPf/zjGrVvHo/H6TBFRERanRI5kSh14okn1ltLl5WVxezZs9mxY4fDEYo4Jycnh7lz59KvXz8mTpwImNq3Xbt2MWfOHPr27etwhCIiIm3LCgQCAaeDEJGm2b17Ny+99BK///3v2bNnDxdffDEzZsxg4sSJeL1ep8OLW6+++iqPPvpojfn/cnNz8fv9NQbLcLlc3HXXXdx8881OhBn3fD4fK1asYMGCBbzxxhv06NGDadOmcdttt9GvXz+nwxMREYkoJXIiMaj2CW337t2ZPn06M2bMYMCAAU6HF3eys7Pp168fTfm5XL9+PSNGjIhAVO2HPQ/jH/7wB3JycnQBQ0REBCVyIjFPJ7mRcfbZZ/P555/j9/sb3GbYsGFs3LgxglHFL7/fzwcffMCCBQt48803SU9PZ/r06cycOZP+/fs7HZ6IiIjj1EdOJMZlZmYya9Ystm/fzj//+U/S0tK44YYbOOGEE5g9ezbfffed0yHGhalTpzY6YbTX62X69OkRjCg+5ebmMnfuXAYMGMCll17K4cOHeemll8jOzmbOnDlK4kRERIJUIycSh+xauvnz57Nr1y5Gjx7NjBkzmDp1KklJSU6HF5MOHjxIRkZGjX5y4SzLYvv27eqr1QK1a9+6devGjTfeqKbCIiIijVAiJxLHap8gp6SkMGnSJO68805OOukkp8OLOZdeeinvv/9+nWTOsizOOOMMPvvsM4cii0179+7lL3/5C8888ww7d+5k9OjR3HXXXVx33XUkJCQ4HZ6IiEhUU9NKkTjmcrkYN24cr732Grt27WLWrFm89957nHzyyYwZM4YFCxZQWlrqdJgxY8qUKfUOeOJyuZg2bZoDEcUev9/P8uXLueaaa+jbty9z587lkksuYf369axatYpp06YpiRMREWkC1ciJtDPhtXRvvfUWHTt25JprruGOO+7g5JNPdjq8qFZSUkJ6ejplZWU11rvdbvbs2UPPnj0diiz67du3jz//+c88++yz7NixQ819RUREjpMSOZF2zG7atmDBArZv3159cj1lyhSSk5OdDi8qTZo0iSVLllBZWQmY2rixY8fy3nvvORxZ9GnoooGa9oqIiBw/Na0Uacd69erFrFmz2Lp1K8uWLWPAgAHccccd9O7dm5kzZ7Ju3TqnQ4w6kydPpqqqqsa6qVOnOhRNdNq/fz9z585l8ODBjB8/nu3bt/P000+zZ88enn32WSVxIiIirUA1ciJSg90EbuHChWzbtq26lm7y5Ml07NjR6fAcV1FRQXp6OsXFxQAkJCSQl5dHamqqw5E5b/Xq1Tz55JO8+uqrJCUlce211/KTn/yEkSNHOh2aiIhI3FGNnIjUkJGRwaxZs9iyZQvLli1j+PDh3H333dW1dGvXrnU6REclJCQwadIkEhIS8Hg8TJgwoV0ncQUFBSxYsICTTjqJMWPGsHHjRp566ilyc3N59tlnlcSJiIi0EdXIicgxHT58mMWLF/O73/2ODRs2VNfS3XDDDXTq1Mnp8CLu/fffZ9y4cViWxeuvv86VV17pdEgRt3r1ahYsWMCiRYvwer1cd911/Od//iennHKK06GJiIi0C0rkRKRZ6juBnzlzJqNGjWrye/z1r39l7NixdOnSpQ0jbTs+n4+MjAzKyso4cOAAiYmJTofUIp988gkFBQX84Ac/aNL2BQUFvPbaazz11FOsX79ezW5FREQcpERORFrEPql/+umn+eabb6pP6q+//npSUlIafF1VVRWZmZmkpqaybNky+vbtG8GoW88999xDYWEhf/zjH50OpUUWL17M5MmTOfPMM/noo48a3dZO3l944QU8Hg/XXXcdt912G6eddlqEohUREZHalMiJyHELP9F3u91MnDiRadOmMW7cuDrbLlmyhB/96Ee43W5SU1N59913GTNmzDH3caisktJKX1uE3yJfrfqSI8XFnHfRxU6HUi3J46Jb0rEn0543bx73339/9eNNmzYxZMiQGtsUFhby6quv8vvf/56vv/663TenFRERiTZK5ESk1di1dPbJ//Dhw5k2bRq33norXbt2BeD73/8+77//PlVVVbjdbrxeL6+++iqXX355o+/95d4CdhcdjcTHaJJAIEDA78fldjsdSrVenRL5XmZag8/7fD7uuecenn766ep1Xq+XO++8k3nz5gGhpPzFF1/E5XJx/fXXN7vprIiIiLQ9JXIi0ibCEwKfz8eECRP40Y9+xJQpU/D7/dXbWZaFZVk88cQT3HnnnQ2+35d7C8gpLkM/WQ3L6JTI2Q0kcmVlZUyZMoU333yzxvcP0LlzZx555BEWLlzIunXrqhPwGTNmkJbWcGIoIiIizlEiJyJt6vDhw7zwwgssWLCA9evX43K56iQSYBK6O+64gyeeeAKXq+7MKErkjq2hRC4/P58f/OAHrF69us5k5rYOHTowdepUZsyYwemnn97WoYqIiMhxUiInIhHh8/no2bMn+fn5DW7jdru57LLLeOWVV0hOTq7xnBK5Y6svkdu2bRvjx48nJyeHysrKel/ncrkYPXo0X3zxRSTCFBERkVagCcFFJCKWLl3aaBIHJtl79913Offcc9m/f3+EIotfn332GWPGjGk0iQPw+/18+eWXbNiwIYLRiYiIyPFQIiciEfHMM8/g8XiOuV1lZSXr169nzJgxbN68OQKRxac33niDCy64gOLi4kaTOFtCQgILFy6MQGQiIiLSGpTIiUiby8nJ4d13322wf1ZtlZWV7N27t0lznEldv/nNb7j66quprKzE52valA0VFRU8//zzHD0aPSODioiISMOOfXlcROQ4ffHFF5x11lkUFRVx5MgRysvLKS4upqKigoqKinpf4/P5KCoqYty4cfzpT39i0IX/EeGoY4/f5+Ouu+7iqaeeqvd5t9tNUlISnTp1IikpiY4dO5KWlkbHjh1JSUkhLS2Nbdu2cdJJJ0U4chEREWkuDXYiIo4rKCigvLyckpKS6gSvsLCQo0ePUlZWRmFhIYPOv5TDeDXYSSNcRfl49u8kKSmJLl26kJycTHJyMp07d6ZTp054vV6nQxQREZFWokRORGKCRq08tsbmkRMREZH4oj5yIiIiIiIiMUaJnIiIiIiISIxRIiciIiIiIhJjlMiJiIiIiIjEGCVyIiIiIiIiMUaJnIiIiIiISIxRIiciIiIiIhJjlMiJiIiIiIjEGCVyIiIiIiIiMUaJnIhImJef/DUvP/lrp8MQERERaZTH6QBERNrSVUN717v+r5tzIxyJiIiISOuxAoFAwOkgRESO5cu9BeQUl9GSn6zC/IPcdM5IABZ9uZnklM6tHV5UyOiUyNmZaU6HISIiIhGgppUiEvdSu6VX34/XJE5ERETaFyVyIiJBhfkH+fidJTx6+/R6H69asYyrhvbm0dunc3Dvnjqv/dsfn6l+/pvPPq5+rrS4iGWvvchVQ3tz1dDevPzkrynMP1j9ulUrlvHo7dMpLS5iwc9nq4+eiIiIHJP6yImIBP3hwXtZtWJZvY+3rFvNmIvG8+yKL5l50el069mLGT+fA5hk7A8P3st5P7ySv27O5ZvPPubnN17DvLeW02/ocF6Y9wj/fOUvPP/J11RWlDPzotMpPnyIGT+fU2MfOdu3csl103jvlb9E/sOLiIhITFGNnIhI0APz/9zg48GnjAYgvVcmAP8MS7a++ewTVq1Yxrk/uAKAk886F4CV//w7AClpXbn0ummkdkuv8/rwffQZcCL9hg6vThBFREREGqIaORGR4/TR398A6o6Q+fr8J7j+7p9y/d0/BeDg3j18+u7bDb6P+u+JiIhIUymRExE5TnbTyMamNFj22ousWvEe02f9jD/P/WWkQhMREZE4paaVIiLAgp/PPu73yN25vd71H7+zhGf+935u/d9H6N1vwHHvR0RERESJnIi0e1vWrWb46d9r8etv++VjAPx7yeuUFhcBoVEsAX577+1AqH+diIiIyPFSIicicc8e6r8+W9at5oFrJ9Bn4Ik1tivMP1jjsZ2g2cvw9z1j7KWA6RM39fShXDW0NzedM5Kzvz8BgDEXjQdMH7nwWrva+xARERFpKisQCAScDkJE5Fi+3FtATnEZzf3Jqj0ASUMWfbmZqacPbXSbv27OrfN+dr+4g3v3sOy1F3l9/hNcet00rpx5Z3UN3M7NG7l34jiuvv0eLptyE0tfeJ7iw4e4cuadzLzo9Or3GnPR+DojZzZHRqdEzs5Ma/HrRUREJHYokRORmNDSRK49USInIiLSfqhppYiIiIiISIxRIiciIiIiIhJjlMiJiIiIiIjEGCVyIiIiIiIiMUaJnIiIiIiISIxRIiciIiIiIhJjlMiJiIiIiIjEGCVyIiIiIiIiMUaJnIiIiIiISIxRIiciIiIiIhJjlMiJiIiIiIjEGCVyIiIiIiIiMUaJnIiIiIiISIxRIiciIiIiIhJjlMiJSMwIBAJOhyAiIiISFZTIiYi0QPbWzezcvNHpMGqwnA5AREREIsYK6BK3iEizTZ06laKiIpYsWeJ0KCIiItIOqUZOREREREQkxiiRExERERERiTFK5ERERERERGKMEjkREREREZEYo0ROREREREQkxiiRExERERERiTFK5ERERERERGKMEjkREREREZEYo0ROREREREQkxiiRExERERERiTFK5ERERERERGKMEjkREREREZEYo0ROREREREQkxiiRExERERERiTFK5ERERERERGKMEjkREREREZEYo0ROREREREQkxiiRExERERERiTFK5ERERERERGKMEjkREREREZEYo0ROREREREQkxiiRExERERERiTFK5ERERERERGKMEjkREREREZEYo0ROREREREQkxiiRExERERERiTFK5ERERERERGKMEjkREREREZEYo0ROREREREQkxiiRExERERERiTFWIBAIOB2EiEg0e/XVV3n00Ufx+XzV63Jzc/H7/fTp06d6ncvl4q677uLmm292IkwRERFpR5TIiYgcQ3Z2Nv369aMpP5fr169nxIgREYhKRERE2jM1rRQROYasrCzOOussXK7GfzKHDRumJE5EREQiQomciEgTTJ06FcuyGnze6/Uyffr0CEYkIiIi7ZmaVoqINMHBgwfJyMio0U8unGVZbN++nX79+kU2MBEREWmXVCMnItIE6enpjB07FrfbXec5y7I444wzlMSJiIhIxCiRExFpoilTptQ74InL5WLatGkORCQiIiLtlZpWiog0UUlJCenp6ZSVldVY73a72bNnDz179nQoMhEREWlvVCMnItJEHTt25Ic//CFer7d6ncvl4uKLL1YSJyIiIhGlRE5EpBkmT55MVVVVjXVTp051KBoRERFpr9S0UkSkGSoqKkhPT6e4uBiAhIQE8vLySE1NdTgyERERaU9UIyci0gwJCQlMmjSJhIQEPB4PEyZMUBInIiIiEadETkSkmW644QYqKirw+XzccMMNTocjIiIi7ZCaVoqINJPP5yMjI4OysjIOHDhAYmKi0yGJiIhIO+NxOgARkVjjdruZPHkyhYWFSuJERETEEUrkRESqjoCvHCoLwVcGvqNAACoK6m5bWQQBH9df1IPiks6QvRgsF3jr6SfnTTXPuRPBnWQeuxLAm9LmH0lERETim5pWikjsC/ihbD+U7TPL8kNQcTh4C94vPwjlB8zjqlKoKIRAlUncWrLLAPgD4G5pT2N3B7C8kJAKriTo0BUSe0CHbpDQFRLSgrfg/cSekJRhlpa7hTsVERGReKFETkSim78CSrKhZAcc2QmlOXA019xKs+HoXpOoBfxhL7LA5TG1YYEABHzmFgssdzBRswB/MPawz2ZZJrlL7AnJWZDcG5IyITkTOvaDTv2hY19wdXDoA4iIiEgkKJETEedVlULRZijaBMVb4cgOKN5iluV5JhmDYHLmNsmNv6rx92wvrLDvJGB/JxYkpkPH/tB5iEnwUk6E1GHQeRh4OjoZsYiIiLQCJXIiEjm+o1DwDRSsN0lbwTdQuN7UrgUCpgbN8pqEJFZq0KKeG9we8FcGa/YsSOoFqSMgbSR0HgqpJ0GXkeBJdjpYERERaSIlciLSNvyVplbt0GpzO/AJHP4aApWAC1zuYK2afoIc4+4Qqt20XNBpAKR/D7qONole19Gmf56IiIhEHSVyItI6yvZB3oeQ9xHsW26SuIDfNIcENYWMFVbw7xWwk7uBkDEOepwHPS6ApN7OxiciIiKAEjkRaamy/bBvmUne9r0PR7ZjBhlJAH+509FJa7KbuxKA5BMgYyz0uBB6jVdiJyIi4hAlciLSdIUbYM/fYfdf4dAq0yqyuomktBsur6ltDfjMIConXGlq7XpeGKrRExERkTalRE5EGhbwmdq27Fdg9xIzB5urg2rcpCbLY2rsvKmQOQH6XQcZl5iET0RERNqEEjkRqSv/S9j1EmxfBBX55oTcX+l0VBIL7P8VbxfoPwX63QDpZ2HmxRMREZHWokRORIzKItj2f7DlKTN/myvBTMYt0lJ2UpecBYN/AoNu1SiYIiIirUSJnEh7d2QbfPsUbHsOfOXB+dv0syCtzHKbEUwH3ARD7jYTlYuIiEiLKZETaa+Kt8La2ZDzpunjpKaTEgmu4AiYvS+DU38NqcOdjkhERCQmKZETaW/K82H9L2HL782Ikz41nxQHWB4z8uWgm2Dkw5CY4XREIiIiMUWJnEi7EYCt82HtLJO8qf+bRAPLay4ojHwYhv63mYRcREREjkmJnEh7UH4QVk6H3HcBv9PRiNRluaD7BXDuS6qdExERaQIlciLxLu9D+GgSVB5WPziJbi4PeFLg3FchY7zT0YiIiEQ1JXIi8Sznb/Dx1aYvUsDndDQiTeACy4KzX4S+1zodjIiISNRSIicSr/I+gvfHmhECNZ2AxBLLgoAFFy2FXpc6HY2IiEhUUq9ykXhUfgA+nIjpDxcdSVxeEcxb6nQULTdvKRSWNv91n30Htz8P1mS46gl44FW4fF7rxxdXAgGwMLXJpXucjkZERCQqKZETiUdr7oOq4qhpTplXBD97HSacFlq3LtskN/bt9udb9t6FpSZZWrii8QTp7TXm+cvnmfvN3WbcSTB1vvksTfXBBvjez+CBKyDwonk8528N7/94FJaa77Eh67JD31Fj2y1c0fjzERPwQ1U5rL7T6UhERESikhI5kXhTuht2vBg1A5sUlsItC2H6+TC4V2j9F9tqbnfZqS17/8ffgXe+ghnPNZwgvbLSJCiLbje3pWvN4+Zsc0oW/M8V5rM0tWZu8edmmdXNLA8vbN5na44PNzf83Lyl8NBiyEiFp280SWV91mWb7zFqBCoh5y0o+tbpSERERKKOx+kARKSV5SwJ9jFyOhDjuX+ZJOisQTXXZ6Q2nFA0x8OTzPJXb9X/fHY+XP80rPwFpCabdbeNg1MfgDMGmtiasg2Yz5CZZj7TvZcdO7Zn3j+uj9ZkhaV1E1Pb7c9DeopJTu3P1tB7vP5528R3fDyw+w0Y8YDTgYiIiEQV1ciJxJuCr03/oiiQVwT3vQgXDa+5PjvfNPF7aLFpFtmWPt1ilr3TQut6dTFLu1awKdvYJp1pPlNjTSzt5qINPa6tsNTUCNrbLVxR9/3tZM3e5qHFoW0efydUGxm+r4cWm+XDkxpP4sAkp3dG5bgiPihY63QQIiIiUUeJnEi8qYyevnGfB5O0QbXmd163yyx/9ZbpQ3b5vOb1PWuOf28yS7t5I0CPzmZpJz9N2cZmf5bPG0lAAy/WrG2s/bi2qfOhuMxss3++2WftJpyzXzHNHvfPh12/M9/dz143z9m1kuH7Wpdttrns1FACePk800+vtg82wDmDQ585qgT8UFHodBQiIiJRR4mcSLxJygDL63QUQKg2KzxBApgwCgoWwtpH4cGJJnFZsrptUbcVSAAAIABJREFUYmiseaOdpDVlG1tqkllu2Xd8cdk+2GD2ccVo87hHZ9MX7+018I91oe3SU+C2seZ5+/tsLO7l680yKx1uvch835lpMPaRmrWgeUWwLa9u09eoYXkgOdPpKERERKKOEjmReNP9vKgZ6KShfmtgmvqdkmVqkxbc0jYjObYFu4nifa3Qvw9CA6KE14YN622WL30aWvfwJJh/k2mW2pRpHOz47P59qcmm3x/Anz8MbbdktUn0olcAup/vdBAiIiJRR4mcSLzp/R+QkHbs7aLINWe2XSI3YVTDz902tunbtJX6atXsZLH2d7JwBdzxp5rTODSHndTZ+3x7DVw6smXvFTGeJDjhR05HISIiEnWUyInEG3cSjPwlWLFTvFOT2y5hspO08D542flmOap/07dpK/Xt2xb+nbyy0vSRe/rGmtM4NMR+bX1TJdj7vHwe9L2r5nx+tqiYS85yw4gHwRuNnfdEREScFTtneiLSdCfeDj3OB5ezfeUeDyYDx5p3rbDUjAbZFuwap+15oXW5h2s+15RtantwYuvEd8PZdfdtf1/h38n1T5tl7f6GDbFfu/Ng3fe192kPjBJ+s7XG1BDHxeWFtNNg2L0OByIiIhKdlMiJxCPLBee9Acl9HU3mBgdHeCw8Glr3ysqaIydm55vJrC8eUfO185aaWqF12cfeT3iiWDtpzOpm+uD9+UPzXGGpub/gllBS1JRtwuMFM79cY8Lj3rLXLMNr3ez7/3GKqSF7ZElo3T/WmRq18O/ErkXLzg+9X/j7hNfszVtqXvvgxJrTFLz2udnuuu81HrvjXF5IzIAL/24GOxEREZE6lMiJxKuENLjkY0gZ7NgolmcGR0K0a7cAOnYwIyfac6EdLqm/j1pBiUlm7LnQGmJNhi63hh53ubVus8BbLzLD8He51Qz1P+nMugN8NGWb8M9yZiOjPFqTzWTitiH3mXU9bw+ts++nJsNzt5rvoOftodjnXFfzPe0pBhZ+AF06miTttrFQVlnz+af+CVPPDa2r/b6Lbie6ubzmAsQln0JiT6ejERERiVpWIBAIOB2EiLQh31FY+1P49mmwLIhwkbdHWLz3spa9/vJ58Lcoal330GKTSLX080hjLOhzOZz1x5gbsEdERCTSVCMnEu/cSTD6KTj7BXM/wk0tb7nQTLj9WSMTaDfks+/MnGrRYl22ud1yodORxBmXF9wd4MwFcP5bSuJERESaQDVyIu3JkR2w5l7IeQtcbvBXRWS3eUVwy0LT1M8eAv9YPtgAfbo2bYTGSNiyF377D/jF1TXnfJPjYLkh4INel8LoJ6HzEKcjEhERiRlK5ETao4Ofwqp74PAqwGVOpttYYSk896/YbZI4b6npe6YkrhVYLtPEt8vJMPq30PNipyMSERGJOUrkRNqtAOx+EzY8AodWgysB/BVOByXxzPJCoNIkcCP+B7Kuian5DkVERKKJEjkRgfzPYdM8yH4jWFtS6XREEk9cXlPrmzkBht4LPc5zOiIREZGYp0ROREJKd8N3z8GOP0PJLtXSScu5vOCvhKTeMOBGGHgLdOrvdFQiIiJxQ4mciNSvcAPsWGQSu4p8MzFzIDKDo0iMcnUAfzl4U6DPROg/DTLGApbTkYmIiMQdJXIi0rhAFexfAXv+DrvfgtLsUFO5gN/p6MRRVnDkySpT89ZnImT+EDLGRXyaCxERkfZGiZyINE/Rtyapy/kbHPzEJHSuBNOvTj8n8c9ubmu5odsZ0OcK6P0D6HKS05GJiIi0K0rkRKTlqkrg4GeQ9yHs/wDyvzAn+aqxixMucHmCiZsXuo2GnmPNYCXdzwFPJ6cDFBERabeUyIlI6/FXQP6XcOAjk9Qd/ByO5prnXB1ME7wIzFknLWEFk7bgiKWJPaDbmZB+JnQ/z9S+uROdDVFERESqKZETkbZVfhAOrTFz1eV/CYe+gNJcIGCmOnB5wVdhHksEWMHmkZVAsMY0qZdJ1LqdDmmjoOtok8iJiIhI1FIiJyKRV1UCRZuhaBMUbjS3w2uhNCdUY+fymH5YvrCEQ5rGsoKTbwdCcwJaLkjuA11OgdQRkDoMUodD56FqIikiIhKDlMiJSPTwV8CRHVCywyyP7ICS7VC0BY7shKri0LaWy0yJAMEmm+0k2bM/twX4qqiR5HpSoFMWpAyBTgOgYz8zd1un/uaxq4NDQYuIiEhrUyInIrGjstDU2h3dZ/rele2Ho3ugLM8kfUf3maac4QlfNcskQC5XsBWnHwiA36G58VweExMuswj4g7HU85Ps6QSJ6ZCYYZKzpAxI7GWaRCZlmKH/kzIhoUtEP4KIiIg4R4mciMShAJQfgorDUFFrWX4Iqo6Ym6/cJIdVpeZxZYGZ0LoymAj6K81zNd66EnzlrM8Bnx9OycL0OXMl1NzOkxRa500x971p4Ek2iZk3NbguxTxOSIMOXc0yodbScrX5NyYiIiKxRYmciEgLTJ06laKiIpYsWeJ0KCIiItIO6TKviIiIiIhIjFEiJyIiIiIiEmOUyImIiIiIiMQYJXIiIiIiIiIxRomciIiIiIhIjFEiJyIiIiIiEmOUyImIiIiIiMQYJXIiIiIiIiIxRomciIiIiIhIjFEiJyIiIiIiEmOUyImIiIiIiMQYJXIiIiIiIiIxRomciIiIiIhIjFEiJyIiIiIiEmOUyImI/D/27jw8qvL+//9rthDWsEQIAUMAkQCCC4jWrUWgqAW1taACgVY0lNYS/dhvwc9l++HT+hVo5VtRKjVW+hNRNq0idWuwWMWtBhRBCCABAgkawpKVbDPz++OeyUIWEkjmzEmej+uaa5IzZ2beWe65z/teAQAAbIZEDgAAAABshkQOAAAAAGyGRA4AAAAAbIZEDgAAAABshkQOAAAAAGyGRA4AAAAAbIZEDgAAAABshkQOAAAAAGyGRA4AAAAAbIZEDgAAAABshkQOAAAAAGyGRA4AAAAAbIZEDgAAAABshkQOAAAAAGyGRA4AAAAAbMbh9/v9VgcBAOFs7dq1Wrhwobxeb+Wx7Oxs+Xw+9e3bt/KY0+nU3LlzNWvWLCvCBAAAbQiJHACcRWZmpuLj49WYj8udO3dq2LBhIYgKAAC0ZQytBICziIuL09VXXy2ns+GPzCFDhpDEAQCAkCCRA4BGSExMlMPhqPdxj8ejmTNnhjAiAADQljG0EgAaITc3VzExMTXmyVXncDiUkZGh+Pj40AYGAADaJHrkAKARoqOjNXbsWLlcrlqPORwOjR49miQOAACEDIkcADTS9OnT61zwxOl0asaMGRZEBAAA2iqGVgJAIxUVFSk6OlolJSU1jrtcLmVlZalXr14WRQYAANoaeuQAoJE6duyoiRMnyuPxVB5zOp268cYbSeIAAEBIkcgBQBNMmzZNFRUVNY4lJiZaFA0AAGirGFoJAE1QVlam6OhoFRQUSJIiIiKUk5OjqKgoiyMDAABtCT1yANAEERERmjx5siIiIuR2uzVp0iSSOAAAEHIkcgDQRFOnTlVZWZm8Xq+mTp1qdTgAAKANYmglADSR1+tVTEyMSkpKdOzYMUVGRlodEgAAaGPcVgcAAHbjcrk0bdo05eXlkcQBAABLkMgBCCslFV7lni63OoyzGnvbHSosKNCRgpKzn2yx7pEedfC4rA4DAAA0I4ZWAggrRwtL9HHWSavDOCu/3y+/zyenK/wTpFG9uyquS3urwwAAAM2IHjkAOAcOh0MOGyRxDjmsDgEAALQAVq0EAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRA9CmrF76B61e+gerw6ihMTHlHc/Vljc2aOGcmSGKCgAAhDO31QEAQHPYu32r3nt1vd5Zs1IT7pqh79w0UQOHjVDilQl6JT3b6vDO29qnHtc7a1ZaHQYAAAgTDr/f77c6CAAIOlpYoo+zTjbpOXu3b9XDd07Sg0uW67of3CZJOpi+S6uXLlba5tRWkchJ0h0JsZLUpJ/HIYdG9o5SXJf2LRUWAACwAEMrAdjee6+ul6TKJE6S4hOG6u7keVaFBAAA0KJI5ADY3vFvj0oyvXDVxScMrfF9ffPMdnyyRQvnzNQdCbF6/W9/Ud7x3Hqfk7Y5VXckxCplwXzlHs2SJG15Y0OtY0HFBfmVj9+REKvUdS82+Pp1PW/hnJnKPphxLr8aAADQSjFHDoDt3Z08T2mbU/XQ7eP0s9/9UdfePEkdOneRVHMY4tOPPKS0zak1npu2OVUL58zUwrUb9fDy57XljQ2659oRlY+PGjO+8jkH03dp1Jjx5tw7J0mSvvfDybruB7cp4YpRmj3mSklS0oJFlc9f+utfatSY7+uV9GzlHc8NxPBPJf/hKXXo3KXOmILP69Grt174LF0dOnfRljc2NNNvCwAAtAb0yAGwvfiEoXrq7S2acNcM/eW3/0eJVyZoyxsbVFyQX+O8h5c/X+u5wZ6wiy8dKalqeObMeb/VK+nZNZ4T7OELnvvOmpWVX0f37lN5LGjHJ1uUtjlVo8dOkCRF9YjWHT+bq7TNqdr2/uZ6Y0rbnKq0zama+JOkyoT0ihvGNOl3AgAAWjcSOQCtQmz8ACUtWKSFazdqwl0z9KeH5ijxyoQ6e7uqm3DXjDqPP7/4d+cd08dv/0OSSeCC+g4YJEn64B9/r/d52/79riTzMwUFEzoAAACJRA5AK3PxpSMrE7pRY8Zr4ZyZDSZz3w8kcsGhi8F5djPn/fa8Y6lru4BgQtZQTGwzAAAAzoZEDoDt3ZEQW2sY5cWXjtR9v31MkhrcRDs+YageXv68TuQc1R0JsVq9dLEeXLJct/70Z+cd16gx4yWpxuImQfX1BAIAADQGiRyAVmH/V1/WOhactxZMqOqStjlVQ0ddpVt/+rPKOXHVtzE4H9dP/JEk6dsjhyqPBRPO79w0sd7n/ex3f5RUexVOAACAIBI5AK3Cgp9M0Y5PtlQmSsHl+yVV7id35rL/kumtS7wyoXJ7gOAtZcF85R3PrfGc4GvX9Tp1HbvihjEaNWa8XvnLk5XHtr2/WRPumqHhV19X7/Muv/57kqTVSxdXbmew45MtleelLJh/Dr8hAADQmpDIAWgVXknPVo+YWH341kbdkRCrxCsTdPjrPXrq7S2Vq01W31Yg+PWS1zbV2WP3zpqVWvvU4zWek3hlQr2vU9exDp276OePLtGoMd/XPdeO0B0JsZKk6Q/9d61zq38d3buPntn8mXr06q3ZY65UyoL5ihuUoFFjxuvBJct15y9/dS6/IgAA0Io4/H6/3+ogACDoaGGJPs46GbL3yz6YoYh27SqHYVY//subrquxD50dOeTQyN5RiuvS3upQAABAM6JHDkCbteWNDYqNH1AriZOkrj2i9eCS5RZEBQAAcHZuqwMAAKt88I+/63RRoS6//ns1krnsgxn66j8fa/yUaRZGBwAAUD+GVgIIK6EcWllckK9t72/Wrs8+rty77cdzHtAlV11TuRiJ3TG0EgCA1olEDkBYCfUcudaORA4AgNaJOXIAAAAAYDMkcgAAAABgMyRyAAAAAGAzJHIAAAAAYDMkcgAAAABgMyRyAAAAAGAzJHIAAAAAYDMkcgAAAABgMyRyAAAAAGAzJHIAAAAAYDMkcgDQivkdfqtDAAAALYBEDgDOQea+dB1M32V1GGdHHgcAQKvk8Pv9VPMA0ESJiYnKz8/Xhg0brA4FAAC0QfTIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzZDIAQAAAIDNkMgBAAAAgM2QyAEAAACAzTj8fr/f6iAAIJytXbtWCxculNfrrTyWnZ0tn8+nvn37Vh5zOp2aO3euZs2aZUWYAACgDSGRA4CzyMzMVHx8vBrzcblz504NGzYsBFEBAIC2jKGVAHAWcXFxuvrqq+V0NvyROWTIEJI4AAAQEiRyANAIiYmJcjgc9T7u8Xg0c+bMEEYEAADaMoZWAkAj5ObmKiYmpsY8ueocDocyMjIUHx8f2sAAAECbRI8cADRCdHS0xo4dK5fLVesxh8Oh0aNHk8QBAICQIZEDgEaaPn16nQueOJ1OzZgxw4KIAABAW8XQSgBopKKiIkVHR6ukpKTGcZfLpaysLPXq1cuiyAAAQFtDjxwANFLHjh01ceJEeTyeymNOp1M33ngjSRwAAAgpEjkAaIJp06apoqKixrHExESLogEAAG0VQysBoAnKysoUHR2tgoICSVJERIRycnIUFRVlcWQAAKAtoUcOAJogIiJCkydPVkREhNxutyZNmkQSBwAAQo5EDgCaaOrUqSorK5PX69XUqVOtDgcAALRBDK0EgCbyer2KiYlRSUmJjh07psjISKtDAgAAbYzb6gAAwG5cLpemTZumvLw8kjgAAGAJEjkAqCiUvKVSeZ7kLZG8pyX5pbJTtc8tz5f8Xt09pqcKirpImeslh1Py1DFPzhNlHnNFSq725ntnhOTp3OI/EmAbvlKpoljylUkVRZKv3JRJSfJ7TZmri/e0Ka91cbaT3B3qfszTWXIELn/cnSSnx5zrbGfKqYvGGUBlJ6vKZqDeq6ofqwmec6Zg/VfjWBfJ4apZJ7ramXKIc8LQSgD25/dJJd9KJd+Y+9ITphIqOymVBb4uzZVKj5nvK4qlsjzJX1H/heDZ3tIv+fyS61xnGrvaSQ6PFBElOdtL7bpLkT2ldj2kiO5SRLfALfB1ZC+pfYy5d7jO8U2BZlKeJ5UcM+Wq/JRUXmDKWXmBVFFQdV92KlAG86SKPFP2Kk6bpC14C0fOiMAtkOS5O0juLqa8RvQwF6CezpK7s7n3dDHl1NPZPNYuWmp3gRTR1eqfBG1Z6THp9DfS6aOBcngiUD8GbiW5UmlOVb1YXmAaNf3loY/V1d6UN09nydVJiow2dWJED1M/Vt53M+UrspfU4ULJ3TH0sYYREjkA4c1XJhVlSkUHpMKDUvER6XS2uRVnBiqokyaZq+SQnG7TGuj3m5ZEv9eqn6BpHK5AouaQ5AvEXu1nczhMchfZS+oQJ3WIldr3kTr0kTrGS536Sx37md4FoLG8p6XiLNMYUpxlylVpbuCWI53+1tyXnjBJXF3lyemR5DD/o36/JJ/kq6h9XmvgCHy+yCE5/IGWnToufh2uQJLXw1yYto+R2vWsSvTa95La9zXH2/epvxcROFNJTqBezJCKDknF2dLpI1LRYel0lknSaiRkDvN/67RZveh0S3IFqkSvaYCtztVeioyR2vc29V/7GFOmOvUP3Aa06h4/EjkA1qsolvLTpfzdUsE+qfCAVLDX3JfmBC4KFUjOXKbyaa0XiE3lqPY7qazgHOaisWN/qctgk+B1HiRFDZG6DGnzLZhtTkWRKUtFB8z96UCiVpRpLvxOf1s1lFGSaQgJJGXy1Z2g4Nw5PVWNTL5ySdUuw9wdAhelfUyDTPvegUaa/lKneHPP0Oy2oyRHytsp5e2WCr+WCvZLBXtMslY5xNFheo/lN4lbm76sdwY+u/w1e/sjupl6sMtgk9h1HiRFDTN1os2TPBI5AKHjPS2d2iGd2mmStlM7TCV1OttUPg6nGW7or7BHS6EtuCSX21ww+n2SHObiMGqY1G2E1CVBirpE6jqC3gDb8psW+fy9VT3XRRnm+8KDZuhjkNMjyRloCKGMhTVH4KLUf0Yy7YkySV3nQeaitGOg56HzIHOxeua8JIS/8nzp5OdS3lemfjz1pbkvzzOPOzymp/vMxB9NEyxT1evDDrFS1PBAfThU6jrc3Jweq6NtFBI5AC3DV2561U5sNbdjH0onvwwM9XBKTlfgYpKPIMu42lX1bjqc5qIw+jtS95Em0es+0rRkIjz4yszQ4ryvpLxdUuF+6cQ283Wwdd7hrNZD62v49WBvwYYvVRuh4HCbhppuI0wDTacBUtRQqdtltu95aDWCdeOxD6VjH0i5H5vhkX5/YISFg17wkHME6sNyM3zT4ZI6DZSirzL1YPeRUo/RgZ7P8EIiB6B5lHwj5bwv5XwgfbPJVFR+X2B8uxgKaRfB1fz8weRuoBQzTup5vdTzu1L7WGvjaytOZ5sW+hOfBxpDPqvquQ4OpfKXk6yhHk7JVUfPQ7eRUo+RUrfLTXLX4UKrA239yk6YejHnPeloqplG4PcGelv9ted8IXwEe8T9XjMXr/uVUu+xUs8xUvTosJiLTiIH4NyUfCt9k2qSt2/eNS2KwQtMX6nV0aE5BYe7ym8u/GLGSj2/J/UeT2LXHIqPmFb5k59Lx9NM4lZ2wjzmigxcjDMMEs0hMP8xOH/I00XqdoW5KO12uemR79jP2hDtrqLI1Ik5mwOJ2y5z3OEJ31Va0UiBhdR85eZap8dVUu9xprGzx1WWrChNIgeg8fK+krL+IR1+RTqRZkZFVg6RRJtRvZWy8yDpwh+ZiqzX96p69FA3v9e0yAeHVR37qFojiJvhxrCGK1LylknymRU1o6+SLrheuuDasB1SFlZKcqSjb0tHXpOy3zLb2jjc9La1eg7T8+0tM3NXe4+X+kyS+t5W996yLREBiRyAevm9pmUxc410eIPpJXC2o8cNNQUvWDxRphKLv0uK+b5tJou3LL908gsp+23p23el3E9Miz1zYRDOglug+CtMktdjtBQzXoq9yfTgsaCKWQH24Crp0Hopb0fNYelomxxuc93kcEm9bpD6TZXiJpue75Z6SxI5ALUc/0w69JKU8YJUdrxqlSfgbIL/K56uUv/pUvxUKfpqmaXs24jS42bY8dG3pCP/MA0g1YenArZTbUimp6vU5xYp9hap9/fNfnhtRXm+lPmylPGcdOzjqmF2QC1O01jncEl9fygNmGnKSzMPvySRA2CU50v7n5P2PmVaGp0RjOfH+QkmdR3ipIt/IV10X+tdBbP4iHRorXRotZnrJklynbEhL9BKONySfJL8UtQIKf5uqd9drXd+3ckvpN1LpMx1gRVhAxveA40RrAvbRUuD5pj6MLJXs7w0iRzQ1hXul/Y8Je3/q+QtDSyqwMcCmpnDZVqvB9wjDU42G7PaXekx0zp/cJVZrCQ4rIaVJNHWON1m2fbuo6QBiVLclGa7ULWOXzr6jrTrD9K3mxmZgubhdEtySP0TpSG/kroMOa+XI5ED2qqCfdIX86Ujr5oLUCoohIIzMMQw9hbpsj+YPa5sxS8d/aeU/ifp6CYzYpTWeSDAYRbA8vukXmOkhAel3jfbb05d9hvStoek/D3VFiECmpHTY/6v+vxAuvzxc27cJJED2prS49LO30l7/2wqXC/DJ2EBh9tc7F10jzTi91JkjNURNayi2PS8pS+R8vfSOg+cTXBj+o7xpudhwMzw35Q8b5e0NdnshRqMH2hJwVWgB98vXfI/TZ5+QCIHtBl+ad9y6Yt5Jnlj/hvCgcNjGhRG/F5K+K/wa7mvKJbSH5fS/59UXijJz9BJoEkcply7IqXBD0hD50mezlYHVZOv1IxQSX9KcjpppEHoOd2Sq4M06imp/4xGP41EDmgLSnOlj2eaJdAZAoZw5HBKF3xXuu6lMOmd80sHV5vhVaW5LCkONAeHW4roKl3+B6n/zPBouCn4WvrgDtMbRzlHOOg/Q7ryacnd8aynksgBrV3O+9IHk6Xyk7QyIrw53ZK7s3TdWrNnlVVObpf+c19g03uHaPwAmptD6jpCuvqvZoEUqxx+Rfp4hqkbqR8RLpxuqWN/6bsbzzp3jkQOaM2OvC5t+bEZCsZYf9hCYO+da16U+t0Z+rfft9zMkZGfBQ6AluRwS/Kb3rmE/wr9++9fIf3n3sAizVwKI8w43GYj8XHvSV2H138aiRzQSuV8IL07lk2IYT8Oh+kJG/Om1HtCiN7Ub4ZR7nkisAolgNBwShfda4aSNfNmyfXKfkP6963Md0V4c7olTzfp5s+lDn3qPiXEIQEIhdJj0vu3q3LD1jCQky8tedPqKM7dkjelvOKmP++Tr6U5KyTHNOmOJ6SH10q3Lmn++FoVv98s67/lx1JxVmje84v50p6lJHHV2L3MNgblOhz4pP3PSWm/DM3bFWdJWyzo7Q/IyZfWfNwy/y+hKLOUmRDyVUhlp6QPp9Tb6EAiB7RG234lVRSEzXDKnHzpf16WJl1edWx7pvlAD97mrDi3184rNhXEs5sbrhQ2bjOP37rEfN3Uc8ZdIiUuNz9LY/3rK+k7/yM9fJvkf9F8v+j1+t//fOQVm99jfbZnVv2OGjrv2c0NPx4yfp9UUSptDcHF3eG/m01/w7B1vnoZcUyrukhb8mbtxzKP1/+8pjqzzIaqnDVFc8Rkx3Kdky/9Zn3V33bNx7Wfl3m86qJ5zgoTY1jze6Wv/yIdWNny77XtAbNqs0Xl/X9elu5e1vz/Ly1ZZqujzISYv1w69onZ/qYOJHJAa1N8WDrwYthM3M4rlu59Vpp5g3Rx76rj/9lf87xbLju313/8DemNz6Wkv9ZfKaz52FRmL8wxtze/MN835ZxL46T/vs38LI1tjVz/qbmP62HuTz7btJ+tKd5Pr/+xJW+aSiwmSlr2E1OR1mV7pvk9hg1/uXTkNbMpb0vxlUmf/ULhWh36X5Q+/l/z9ePTpIduMV8/dIt06ElJHBYnAAAgAElEQVTpZ2Orzgv+nwW/T7nXPKe+v3d96iqzoSpnTdEcMdmtXOfkSxk50u8nm7/r6vtNUlC9FyavWNp+SFp+j3TqWem7Q6Sxj7XMRXOz8ktKe0CqKGy59yg6aBpuLKwfl9/T/K/ZkmX2TJQZi+z8v3UeDs+aC8C5O7LBzDEKE399z3zwX31RzeMxUeZDNXibdMW5vf7vJ5tbfTKPmw/t/75Niupgbj8bZyq37ZmNP0cyP0OfbuZnaoy/vHtuP1NT5RXXX/nOWSGdKjKV9KQral7sn/kaL3/acjGeO7e58Gop2W9Kpd8qnFemvPoik7D9e3fN43E9TAu3JO09WvOxvGLz/zv5qqa/X11lNpTlrLGaIybJXuU6I6fm3+Wu75j7X1VL1t9Pr/o8jepQdU74D2PzSxV50uFXW+4tjrweHlseNLOWKrP1ocyEmk8q2Ftno2br+28G2rpTX5r5RWEgJ998WI4ZWvN45nHzAfmb9WboR0v6aK+5j+1Wdax3V3Mf7BVszDlBk68yP1NDw0rOHM52tuFtecWmZTR43rOba79+sIIKnvOb9VXnPP5GVcth9ff6zXpz//vJpnJqyF/fk34ZqnVFmsQrnfqi5V4+9xPJEdFyr99MZt5g/sZnthBnnzT32w7WPL472yR/1RP336yv+p+oT31l9myau5w1h9ZYrs9sEAv2iDxye9Wx+hrFgr234c0l5dYx7q25nPoyXKaNS6qa0xYczld9eLRkfZltCGUm1Bzm//cMJHJAa1MePnPjPg0kaRedsb/z9kPm/tHXzLj5W5c0bbx9UwR7Mapf0PbsYu6DH/iNOSco+LN82kACGuxlrO/7MyUulwpKzDnfLjfveeawlflrTEvpt8vNkLpHXzPzIaSara7B99qeac655bKqSu/WJXWP+//XV9K1F1f9zGHF75PK8lru9ctOSgqP8tKQ4MXIma3Nb3xuLjZe+qj28Zk3NP196iuzZ9Pc5aw5tMZyXV3mcXPhKkmJ19UfR/D9znX4ekj5K6SyEy33+hWFYVM/Sqa36KFbzN8/66TUb27T68KWLLMNocyEmMNpru/OQCIHtDbtYySHx+ooJFW16p05nG/SFWYc+hcLTavYxm3Shq0tE0NDQzqClVVjzgmKam/u935zfnEF/esr8x63jTTf9+xihrps3Ca9tb3qvOjO5oK9Z5eq32dDcW/aae7joqX7xpjfd59uZtx/9V7QnHxpf07tVsuw4XDXu+xys+hwoW2GWqXca/4vgsMo84pNwv5w4P8lOBwqr9hc3AyJrfn8sw21kuovs2fT3OWsObTGch2Uedxc9D/6mvl+4+f1n7v1gPnMvSHh3H6WkHJ6pI79Wu71I3uZ9wgTwc/dnl2kx6ear6vXhVaX2YZQZkLM7zXXd2ewR+0FoPEuuD5sFjoJfmDWJaqDGdP/+8lVF6h2EByi+KsGWhWbIjgJvHpvWPACvHovy+8nm4nYmccbt7x0ML5L48x9cP6DJD3/ftV5G7aaRC98+aULzqFrqbH6TJS8ZS33+s3ou4GLiuAwyt3Z5m8X18NcdAQv6HZnmzJ1tuG0dWmozLZmdinXQXE9TG9DsDHsVy/WP0/2iber5kGFPV+ZFDux5V7/guvCpn48U3CRkqYuOGVVmaXMhJjDKUV/p9ZhEjmgtYm9WYrodvbzwsiUq1oukWtoEZXg+PfGnNNS6mpJDFYeZ/5Ont0s3f//1dzGoSmCSV3wPTdukyaMOLfXChl3e+nCH7bc63e7TIoZG1at9PW5uLf5f7x7mfn+jc+lqwIt+lOvqboAfONzafTA0MYWjuWsLZTrS+OqhojVlQCs+dj8HsK2x706p1vqMVrq2cCYt/MVe4vkCccx5KFnZfmoC2WmAU6PaXSs49qORA5obVztpRG/s81wMSnQW9RCFUewsqo+7yA4ofyK/o0/p6XU9d5B1X8naz42lc6yn9TcxqE+wefWtTx08D1vXWKGmtS131hY7CXncEnDHmn5C6/Rz0iuSFuUmeAqlMG5jsHW6yviq45vz6xK2kMlHMtZayzXdanvedszpa+OhHuPe5DDXKxevUItulqXu6M07L/NZ0uYClUSZWX5aGw8QW2+zPi80vAFdT4U/rUWgKYbNEfqeYPlvQyPB5KBs+01k1d8bsukN0awxykjp+pYcKW/4GONOedM1Ve8Oh9Tr6n93sHfV/XfSbAXprHzIILPPZhb+3WD71l9+4e6JqVbyumRul0uDXmo5d+r00DphtfMfLwwvsCTpJGBC6wn3pZ+UK01OnhhMvax87sIaWyZPVNLlbPz0RrLdV2Cr7v6/qpjOflmnmz1+VXbM83KiGHH4TTl7rr1UtSwln+/If8ldR1hef14puAc1+8OadrzWrLMNgZlpoU5XNKw+aY+rAOJHNAaOZzS9X+XOvSztLK6ODAvN+901bE1H9dcOTHzuNnD5cYz6u/gksyN2V+qegV2ZmUW18PMF3r+ffNYXrH5OuXeqoqgMedUj1c6+9C16nEHF6eo3tIY/PrmS01L5GMbqo69td20QFb/nQRbKzOP19wzLPic6q2ZS940z33k9ppLM6/71Jx3V+1h9uHF6ZEiY6Tv/cMkV6HQ60bpxk2Su0PYLBZUl6gO5sJt47baQ39S7jX3V9UzJKgxS5nXVWaDQlXOQln2g+xSrm9dYu6D8eYVm1X4Hrm9qlzn5JuV/n71Ys3e9sseDrNV+CRTvl2R0pg3pdgfhO49v/cPKaKHZWU9+HcN1oU5+aZsPj6t5uez1WVWqr88UmZCwOmWYm8yo6zq4fD7/WG0owaAZlXyrfTuWCl/r+QP/QTvnHyp1xzp4/+tuujcuK1qk81Hbpd+fFXdw8B+s17KLTBLMr/eQKdMfUMAz+xRCr7vpCukB26qnTg29pxPvjZbJny7vP7l+hs7LDEYY06+WXQkOF5/9f2mUqs+0Xp7pqlUHrnd7Pf21Dvm9/PwbabSPfPxYGzPbq563ZR7zXzEhiZwB2O3rEfO6TENEOM2Sx36hv79izKlD34sndxqtj4IQ9szzcImZ/a8bc80m7rXt8pd9X0F61NXmZVCW86sKPt2Kdeffl1zk+LHp5mtQ6r/reasqH8Vvz2Pn/twtGbncEtdEqQb/i51HhT69y/cL6XeIJUcs6R+/NdXpmd94zaTrEy+qu6yIFlbZusrj5SZFuZwSb0nmPLhbFf/aSRyQCvnPS198WtpzzLJ4ZBCXOSDq0o9dMu5Pf/WJQ1fzIXab9ZLXTue+8+DhjikvrdKV//N2gV7/BXS/hXS5/Mlb4Hkq7AuFgucb5ltLqEs+5TrEHJ4JKdLGjpPGvZwgxepLa70mPThVOmbf0kKz4abxghFmT2zPFJmWojDaRoRB/9SunzJWUdVMbQSaO1c7aWRT0nXrDJfh3io5b3fMxuPftLApqH1+eRrswRwuNieaW73fs/qSFoZp0dytZOuSjFz1axeddXhli5Kkm7dJw28T5Iz7ObTtKTzKbPNJZRln3IdIk6PaUwckCjddtAs3mBlEidJ7S6QxrwjjVhgLqCdIRrK3cxausyeWR4pMy3E6ZHcXaTvvSmNfLJR9Q49ckBbUnhA2vaQdOQ10yIaop6G4Pjz309u/Gp6//pK6ts9fIYB7T0q/ekt6X9/XP8wEjSRw2U2Oe09QRq5VOoy2OqI6pb3lbT7j9KBl8yCemG6D1VzOpcy21xCWfYp1yHg9JiRIHE/Nr1w3cJtol7AsQ+ltF9IJ3dI8gdu9tFSZfbM8kiZaQHBujBuinTF/5M69Gn8U0nkgDYo9yMp7QHpZJokp/kAaWF5xdJf37PvMIwlb5q9Z6i4moHDaS7sug6XRv7JLDRiByU50td/kdKflMpPSnKEpOxYxe5ltjEo1y3E4ZLkN70Lg+83Kym3j7U6qrPz+6QDK6XP/49Ufsp2w6pDUWYpM83JKclvVqS88s9S9NVNfgUSOaDN8kuHX5W+ekw6sVVyRki+MquDQmvm8JhFBboON3s5xU2xxd5ttfjKpMx1UsZK6dt/ySR0Ptl5jg1w3hwOVV6YXnC9NOCnUr87zaqUdlNRKO1ZKu3+k1R2SqZ3jvKNZuJwm7nYUZdIlzwi9Zuic90/kUQOgHT8U2n3Einz74HektY/dAwh5PSYnqs+k6SEh6Se11sdUfMpPSZlvmxa8Y9/anoifCR1aCscVRel3UeZ+W9xk832Ia2Br1Q68IK06w9SwT4zh85mvXQII06P+f/pPV4a8msp5vx3fyeRA1Cl+LD09V+lA89LRYfopcO5c3rMXLL2sdKAn0gD75U69bc6qpZVfFg6/Hcp6x9SzgfmItDVTvKWWh0Z0HyCZdvhkS64VuozUYq7Q+oYb3VkLcgvZb9l6sesf0gOnxkeHqZblCCMONySvGaYcf8ZZiGtrpc038uTyAGoU95XpiXy679KZcerWl2B+jjbmeTF01nqe7uptGLG6lyHjNia97RZPOGbTebCL++rwDBSJ+UI9hJcrMRfYfZ2jL1FihlnFinytMGJUmWnpKzXpf3PmQYbp0vyeWW3xVHQghzB1U8dpvdtwE+kvreZxvHmfisSOQAN8ldI3242F6OHX5OKM6uGytEa2cY5AqttVZiet763m9b5mHFtarn+Rik8IOX8Wzq2Rfr2Pakww+S3zgh67BBeHJ5AY4Nf6hAnxYwxc956flfqfNFZn96mFOwzc80Pvyqd+I/kdwT2a6Wxps0JDrt1RUq9vy/1/aFJ3lp4Ox0SOQBNk7/HJHVHXpdyPzQJnTPCzKvj46T1Cw63dbikHqNNRRX7g2YdKtImlOZKxz4yiV3Ov6UTn5sy5HAF5tkxpBkh4IwINMp5zf9dt8tMwnbBdWbYZGRPqyO0j9JcKesN01t39C2p4nTNpBiti9Md6Kn2SpG9pAt/KPW5Veo1JqQL/JDIATh3FUVS7idSzvtm9b7j/zEXoPTYtRKBDXJ9ZeaCpMdIqddYs1jJBddK7k5WB9h6+MqlvF3Syc/N7fhn0qntUkWxJEe1xhLKFM6FQ3JFSN4ySX7J1d6sHttjtEneul1uGmNaYOhXm+SvkI6nmUaab941DTbeQGIn6kZbqp64tYs2QyZ7fs80fFi4ByqJHIDm4yszF6DHPjBJXe6n0uls85iznancWvG+W/bmCCRtgRVLI3tKPa6Soq8yw6p6jLbnMuK25pcK9geSuy+k/HTp1JdS4cGqoVuudqZMsZIeJJnGF1dVOXa4pA4XSt1GSFHDpK6XSt0vlzpdZM+tP+zKX2G2+cl53zR+5n4snT5qHnNFmrqT5C58OFwy85nLTTnpNFDqeZ0UfU1giPEgqyOsRCIHoGWV5kontplK7PhnZh5BcbYkv/mAdHqqWokRAqZ3x+ct17YDPo0aIKl9b5Oo9bhS6naF1H0kQ6rCmb/CzLHL222Su/w9JtEr+FqqKKg6L9i7wjDN1qWuv6u7k5m/1nWEFDVE6jzY3HcayHzVcFWSI534zPTc5X4infhMH+44rpNF0sQr3KZ+pOy2sEBPtS8w2sHhMPNCL7hG6j5a6jHK9Fa7O1odaL1I5ACEXkVR4AJ0txlOlrfLXIgWH6nqsXO6TauYt1zsydVEDkdgboa/ak9Ah9OsONf1UilqmN7b7dCYqQt1xeWX6Wdzfq6pU6eqY8fwrazQSOV5pseu6EDVfcF+qXCfVHTYDO8KcrgCw4V8VT04sJbTo8qVTauPXnBFmvLb+WKTnHWKlzr2r7qP6GpRwDhfhYWFevHFF/X000/ryy+/1B0Tv6uX/3S3dOor0wN/aqdZOVqSaYjzmDLLgiqN54yQ5K/WU+2UOvSRul5mhhRHDZOihkpdEsywYxshkQMQPnxlZnW/ogPmvvCAVJQh5e81F6XVexsczqolfv0VbWdYSvDndkjyVqhGkuvuLHWKM63xnQaYfZ069Q/cBpjhrdVs3bpVKSkpWrVqldxut+666y794he/0IgRI0L5EyGUSnPNcOfiI9Lpb6TTgfviw4FbllR2onZ5cnrM/57fHxjG2UbK2/mq/JxySKojYXY4pIjuZtXXDnEmWWvfO3AfI7UPfE8Peauzd+9erVixQs8++6yKiop06623KikpSePGjat9ctmJQINMRtUtf69U+LVU8k3N8uqMMP9X3gpJbWAqQ7DR1+etmdw6I6SOF5r6sPNFpg6svA1sNVMFSOQA2Ed5XrUL0Gyp5FvpdJYZolJ4wBwvza2Z8FVymAsqpzMwitMn00JnUaumM3hx5zR3fl8gljo+kt2dpMhoKTLGJGftY6TI3uYCr32MuQhs3+ecW+VPnTqldevWaenSpdq1a5dGjhyppKQkJSYmqn17e7VOohn4vaZslR4P3I6ZclV6vNp9TuCcY1JZvullr294dHAlTofDnOLwB3qL/eHbq1BZPgO3YBn1+xqe5+vuZPZRjLxAatfTrGbXLlpq1yNwi5baXVD1dWSvwHwctAVer1dvvvmmnnzySb377rsaMGCA7rvvPs2aNUvR0dHn9qK+8kC9mB24Ha26L8o0j5Uek8rzVauM1mhokKRG/I+3FEdgOGmwvPl8VSNKzjwvIsrUfZWNH7GB+rD6fWyofwJLkMgBaIX8UukJqeykacmsfl96QqooNDdvqUkOK4rN9+WnzIbW5YFE0FceWDWw+kuX1973yxlRe7U3d/uqY57O5mtPN8ndIXCxFxU41tl8H9FNatfd3EeccR+iRQn8fr/effddpaSk6LXXXlOnTp2UmJio5ORkDRgwICQxwMYqCs3FYnmBaUwpOxX4Pt98X15gvvZXmK/93sD3PpMgSlVDyMpOqfKi01cRSBTreU9JO49IXp90aVzguLuj6tyI3t2h5pyx4B5PET3Mfbvu5kLR09ncuzuZ8z2dTY+3p7PZBNsTFbgFvg8+BtTh6NGjWrlypf785z8rKytLN954o5KSkvSjH/1ILlcIE/myE4G68UTtryuKTX3oLTXlquyUGYpdfso85gvUe+WFtRO9QDms5HDVHqLoalfVC+buZL72dDVfuzuYhkhnpPnaExWoD3sE7gO3dt1ZLfkMJHIAgFqCFx5PP/20jhw5Unnh8cMf/lBut9vq8IAaEhMTlZ+frw0bNlgdClBpy5YtevLJJ/Xaa6+pW7du+ulPf6rZs2erf//+VoeGVoK1ZwEAtfTu3Vvz5s3TgQMH9M477ygyMlJ33nmn+vXrp/nz5+vw4cNWhwgAYSc/P18pKSkaPny4rr/+emVkZGjZsmU6ePCgFi1aRBKHZkUiBwCol9Pp1Lhx47Rx40bt3btXiYmJeu6559S/f39NmjRJmzZtEgM7ALR1u3fvVnJysmJjY5WcnKzLL79cn3/+udLS0pSUlMR8Y7QIEjkAQKNcdNFFWrRokY4cOaLVq1erpKRE48eP1+DBg7V48WIdP3787C8CAK1EWVmZ1q9fr/Hjx2vo0KF6++239Zvf/EZZWVlauXKlLrvsMqtDRCtHIgcAaJJ27dpp8uTJSk1N1a5du3TzzTfr0UcfVZ8+fTRlyhRt2rTJ6hABoMVkZWVpwYIF6tu3r+6++25FRkYqNTVV6enpmjdvnrp37251iGgjSOQAAOdsyJAhWrp0qbKzs/Xkk09q7969Gj9+vEaNGqWUlBQVFdWz2iAA2IjP59OmTZs0ZcoUxcfHKyUlRffcc48OHDigjRs3aty4cXI46lgpFWhBJHIAgPPWuXNnJSUl6YsvvlBaWppGjhypBx54QLGxsZo9e7a+/PJLq0MEgCY7deqUli5dqkGDBmn8+PHKzs7WSy+9pEOHDmnRokW68MILrQ4RbRiJHACgWY0cOVLPPPOMsrOz9cc//lEffPCBLr300speutOnT1sdIgA0aOvWrZo9e7b69Omj3/72txo3bpx27NihLVu2aPLkyfJ4PGd/EaCFkcgBAFpE165dlZSUpJ07dyo1NVUDBgzQL37xC/Xv31/z58/XgQMHrA4RACqVlJRo5cqVuuKKKzRq1Ch98MEHeuyxx5Sdna1nnnlGl1xyidUhAjWQyAEAWlRwC4N169YpMzNTDz74oFavXq2LLrpI48eP1/r161VRUWF1mADaqH379mn+/Pnq27ev7rvvPl100UWVizklJyerY8eOVocI1IlEDgAQMsGNxjMyMurcaPzIkSNWhwigDQguXjJp0iQNHjxYq1at0v33368jR45o3bp1GjdunNUhAmdFIgcACDmXy1W50fiePXsqNxofOHBg5RYGbDQOoLl98803Wrx4sQYMGKAJEyaopKREa9eu1cGDB7VgwQJdcMEFVocINBqJHADAUoMGDarcaHzVqlU6efKkxo8fr4SEBDYaB9Astm7dqhkzZiguLk6LFi3Sbbfdpn379ik1NVWTJ0+W2+22OkSgyUjkAABh4cyNxm+66SY9+uij6tu3r6ZMmaIPP/zQ6hAB2EhBQYFSUlI0YsQIjRo1Srt27dKyZcuUlZWlpUuXasCAAVaHCJwXEjkAQNgJbjQevODau3evrrvuOjYaB3BW6enpSk5OVmxsrJKTk5WQkKCPPvpIaWlpSkpKUocOHawOEWgWJHIAgLDVpUuXBjca37Fjh9UhAggDZWVlWr9+vcaPH6+hQ4fqrbfe0iOPPFK5eMl3vvMdq0MEmh2JHADAFqpvNP673/1OmzZtqhwytXLlSpWXl1sdIoAQy87O1uLFizVw4EDdddddkqQNGzZoz549mjdvnnr06GFxhEDLIZEDANhK165dlZycXLlQwYABAzRr1ixdeOGFbDQOtAHBrQOmTJmifv366YknntC0adOUkZGh1NRUTZo0SQ6Hw+owgRZHIgcAsKXqG40fOnRIDz74oF566SU2Ggdaqby8PKWkpGj48OEaP368MjIy9NxzzykzM1OLFi1Sv379rA4RCCkSOQCA7cXGxmrevHk6cOCAXnvtNUnSnXfeqfj4eDYaB2xu69atmj17tmJjY/WrX/1K1113nbZv3660tDTNmDFDHo/H6hABS5DIAQBaDZfLpUmTJik1NVV79uzR9OnT9de//pWNxgGbKS0t1fr16ytXq33//ff12GOPKTs7W88884xGjBhhdYiA5UjkAACtUnCj8aysrBobjQ8ZMkSLFy/WiRMnrA4RwBm+/vprzZ8/X3379tX06dMVGxtbubdkcnKyOnXqZHWIQNggkQMAtGrVNxrfunWrvvvd7+rRRx9Vnz59NGXKFH300UdWhwi0adUXL0lISNALL7ygWbNmaf/+/Vq3bp3GjRvH4iVAHUjkAABtxhVXXKFnnnmmcqPxPXv26Nprr2WjccAC3377beXWARMmTNDJkye1evVqHTp0SIsWLVLfvn2tDhEIayRyAIA2J7jReHDBhOBG43369NHs2bO1c+dOq0MEWq3g4iX9+/fXwoUL9f3vf187duxQamqqJk+eLLfbbXWIgC2QyAEA2rTgRuMHDx7Uww8/rNTUVA0fPpyNxoFmVFBQoJSUFF122WUaNWqUtm7dqieeeKJy8ZKhQ4daHSJgOyRyAABI6tmzp+bNm6evv/66xkbjcXFxmj9/vg4ePGh1iIDt7NmzR/Pnz1e/fv00d+5cXXzxxdqyZYvS0tKUlJSkDh06WB0iYFskcgAAVHPmRuMPPPCAXnzxRQ0cOLByo3Gv12t1mEDY8nq92rhxY+Uqsa+88ormzZunI0eOaN26dbr22mutDhFoFUjkAACoR3Cj8f3792vNmjWSzEbj/fr104IFC5STk2NxhED4OHr0qBYvXqz+/fvr9ttvlyStXbtW6enpmjdvnqKjoy2OEGhdSOQAADiLiIiIyi0M0tPTNX36dC1btkwXXnghG42jzduyZYumTJmiuLg4/elPf9LUqVO1f//+ysVLXC6X1SECrRKJHAAATXDxxRdr0aJFOnLkiFatWqXs7Gw2Gkebk5+fr5SUFF1yySW6/vrrlZGRoT//+c86cOCAFi1apPj4eKtDBFo9h58mRAAAzsvWrVuVkpKiF198UV6vV5MnT9aDDz6oyy+/3OrQWp21a9dq4cKFNeYpZmdny+fz1dh3zOl0au7cuZo1a5YVYbZa27Zt0zPPPKMXX3xRTqdTd999t37+85/r0ksvtTo0oM0hkQMAoJnk5+drzZo1WrZsmXbs2KGRI0cqKSlJ06dPZ3W+ZpKZman4+PhGDWXduXOnhg0bFoKoWrfS0lK9/vrrSklJ0aZNmzR48GD99Kc/VVJSkrp162Z1eECbRSIHAEALCPbSrVy5Uu3atdOdd96puXPnklg0g2uuuUaffvqpfD5fvecMGTJEu3btCmFUrU9GRoZSUlK0YsUKnThxQjfffLOSk5M1duxYORwOq8MD2jzmyAEA0ALq2mj8kksu0XXXXaf169ez0fh5SExMbDCR8Hg8mjlzZggjaj18Pp82bdqkKVOm6OKLL9bKlSt1zz336MCBA9q4caPGjRtHEgeECXrkAAAIAZ/Pp3/9619KSUnRq6++qujoaM2cOVNz5sxRv379rA7PVnJzcxUTE1Pvfn4Oh0MZGRksuNEEp06d0vPPP6+lS5fqwIEDuvbaa5WcnKzbb79dHo/H6vAA1IFEDgCAEMvKytKqVau0bNkyZWdn68Ybb1RSUpJ+9KMfsVR7I02YMEHvvvturWTO4XBo9OjR+uSTTyyKzF6CQ4BfeOEFeTwe3XXXXQwBBmyCoZUAAIRYnz596txofPDgwVq8eLGOHTtmcYThb/r06XUueOJ0OjVjxgwLIrKPkpISrVy5UpdffrlGjRqltLQ0PfHEE8rOztYzzzxDEgfYBD1yAACEgb1792rFihV69tlnVVhYqNtuu01JSUksLFGPoqIiRUdHq6SkpMZxl8ulrKws9erVy6LIwte+ffv03HPP6dlnn1VRUZFuvfVWJSUlady4cVaHBuAc0CMHAEAYCG40Hhx2GdxofOjQoVq8eLFOnjxpdZZe2MAAABrVSURBVIhhpWPHjpo4cWKN+VtOp1M33ngjSVw1Pp9PGzdu1Pjx4zV48GCtWrVKv/zlL3X48GGtW7eOJA6wMRI5AADCSGRkpCZPnqwtW7YoLS1NN9xwg37/+9+rX79+mj17tr744gurQwwb06ZNU0VFRY1jiYmJFkUTXr755hstXrxY/fv31+233y7JbKZ+6NAhLViwQBdccIHFEQI4XwytBAAgzOXl5Wnt2rV66qmntHPnTjYaDygrK1N0dLQKCgokSREREcrJyVFUVJTFkVln69atWrp0qdasWaOOHTtqxowZeuCBB9S/f3+rQwPQzOiRAwAgzEVFRSkpKUk7duxQWlqahg4dqvvvv199+vTR7Nmz9dVXX1kdoiUiIiI0efJkRUREyO12a9KkSW0yicvPz1dKSopGjBihUaNGadeuXZUroi5dupQkDmilSOQAALCRkSNHauXKlTp8+LDmz5+vf/7znxo+fLjGjx9/ThuNl5aWtlCkoTF16lSVlZXJ6/Vq6tSpVodzzkpLS7Vv374mPSc9PV3Jycnq06ePkpOTddlll2nbtm1KS0tTUlKS2rdv30LRAggHDK0EAMDGqm80/ve//109e/bUjBkzGr3R+OOPP669e/fq6aefltvtDkHEzcvr9SomJkYlJSU6duyYIiMjrQ6pyfLy8nTrrbeqS5cu2rhxY4PnlpWVacOGDUpJSdGmTZs0aNAgzZo1S/fdd5+6d+8eoogBhAMSOQAAWomsrCw9++yzWr58uXJzc8+60bjf71f//v116NAh3XTTTXr55ZfVsWNHCyI/Pw888IDy8vL0t7/9zepQmiy4Oml6erok6cCBA4qLi6t13pl/21tuuUXJyclsTwG0YSRyAAC0MtV7bd59910NHDhQ9957r+65554aqxX+85//1IQJEyRJbrdbw4cP19tvv62ePXs2+PonSspVXO5t0Z+hKT5P+0yFBQW6fsyNVodSqb3bqR7tIxo8Z/fu3Ro3bpyOHTum8vJyeTwe/frXv9ajjz4qqWZv66uvvqro6GjNnDlTP//5z+tM9gC0LSRyAAC0Ynv27NHf/va3OjeBvu222/TWW29VzqvzeDzq1auXNm3apMGDB9f7mp8dPaXD+adD9SOcld/vl9/nk7OOXker9O4Uqe/06Vbv459++qluuukmFRUV1ZjX2K1bN+3evVsbNmzQE088od27d2vkyJGaO3eu7r777hr75gFo20jkAABoAwoLC/XSSy/pL3/5iz7//HMNHTpU6enp8vl8Nc5zu93q0KGD3nzzTV177bV1vtZnR0/pSEGJuISoX0ynSF1TTyL3+uuva8qUKaqoqJDXW7Nn0+FwVP4NgnMdhwwZEoqQAdgMiRwAAG3Mp59+qtmzZ2vnzp21EglJcrlccrvdWr9+vSZNmlTrcRK5s6svkVuxYoXuu+8+SaqVREuS0+lUfHy8tm/frk6dOrV4nADsi+0HAABoY0aOHKmjR4/WmcRJZiXIsrIy3X777Vq+fHmIo2u9Fi9erFmzZsnn89WZxEkmucvIyFBGRkaIowNgNyRyAAC0Ma+//rqOHTvW4Dl+v18+n08///nPlZycTO/befB6vUpKStLDDz/cqPM9Hg8JNICzIpEDAKCNWbZsmZzOxl8CPPXUU5o5c2aTNxuHVFRUpIkTJ2rFihWNTobLy8v1/PPPKz8/v4WjA2BnJHIAALQh+/bt03vvvVc5rNLtdisiIkLt2rVTu3bt5PF4aiV5fr9fL7zwgm6++WYVFBRYEbYtHTt2TDfccIPefvtt+Xy+Wr/nhrhcLr3++ushihSAHbHYCQAAbUhubq727dunwsJCnTp1Svn5+SosLFRhYaEKCgp06tQpFRYWKj8/X3l5eTpx4oQKCgpUVFSkkpISDRo0SP93xWqVdIhiuGUDuqhCf/vdfH3zzTfq1q2boqKi1LlzZ3Xp0qXy1q1bN3Xp0qXW8a5du1odPgAbIJEDAABN8uGhb5VT6ieRa0BD2w8AQHNgaCUAAGiSiIh2VocAAG0eiRwAAAAA2AyJHAAAAADYDIkcAAAAANgMiRwAAAAA2AyJHAAAAADYDIkcAAAAANgMiRwAAAAA2AyJHAAAAADYDIkcAAAAANgMiRwAAAhLq5f+QauX/sHqMAAgLLmtDgAAALQNdyTE1nn8lfTsEEcCAPbn8Pv9fquDAAAA9vHZ0VM6UlCic7mEyDueq3uuHSFJeuGzdHXo3KW5wwsLMZ0idU2fblaHAaAVY2glAAAImage0ZVft9YkDgBCgUQOAACEnbzjudryxgYtnDOzzu/TNqfqjoRYLZwzU7lHs2o99/W//aXy8R2fbKl8rLggX6nrXtQdCbG6IyFWq5f+QXnHcyufl7Y5VQvnzFRxQb5SFsxnjh6AsMUcOQAAEHaefuQhpW1OrfP7vdu3atSY8Xpm82eaPeZK9ejVW0kLFkkyydjTjzyk6yf+SK+kZ2vHJ1u04CdTtOS1TYpPGKpVSx7TO2tWasWHX6q8rFSzx1ypgpMnlLRgUY33OJKxT9+/a4b+uWZl6H94AGgEeuQAAEDYeXj58/V+f/GlIyVJ0b37SJLeqZZs7fjkQ6VtTtV1P7hNkjT86uskSR+/8w9JUudu3TXhrhmK6hFd6/nV36PvgEGKTxhamSACQLihRw4AALQaH/zj75Jqr5D58vIndHfyr3V38q8lSblHs/TR2xvrfR3m7wEIdyRyAACg1QgOjWxoS4PUdS8qbfM/NXPe/+j5xb8LVWgA0KwYWgkAAMJKyoL55/0a2Qcz6jy+5Y0N+stv/4/u++1jio0fcN7vAwBWIZEDAABhY+/2rRp65XfO+fk/+90fJUn/3vCyigvyJVWtYilJf3pojqSq+XUAYFckcgAAIGSCS/3XZe/2rXr4zknqO3BQjfPyjufW+D6YoAXvq7/u6LETJJk5cYlXJuiOhFjdc+0IXXPTJEnSqDHjJZk5ctV77c58DwAIdw6/3++3OggAAGAfnx09pSMFJWrqJcSZC5DU54XP0pV4ZUKD57ySnl3r9YLz4nKPZil13Yt6efkTmnDXDP1o9i8re+AOpu/SQ7eP04/nPKBbpt+jN1etUMHJE/rR7F9q9pgrK19r1JjxtVbObIqYTpG6pk+3c34+AJwNiRwAAGiSc03k2hISOQAtjaGVAAAAAGAzJHIAAAAAYDMkcgAAAABgMyRyAAAAAGAzJHIAAAAAYDMkcgAAAABgMyRyAAAAAGAzJHIAAAAAYDMkcgAAAABgMyRyAAAAAGAzJHIAAAAAYDMkcgAAAABgMyRyAAAAAGAzJHIAAAAAYDMkcgAAoMn8fr/VIQBAm0YiBwAAbC1zX7oOpu+yOowaHFYHAKDVc/hpUgMAADaWmJio/Px8bdiwwepQACBk6JEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbIZEDAAAAAJshkQMAAAAAmyGRAwAAAACbcfj9fr/VQQAAADTG2rVrtXDhQnm93spj2dnZ8vl86tu3b+Uxp9OpuXPnatasWVaECQAtjkQOAADYRmZmpuLj49WYy5edO3dq2LBhIYgKAEKPoZUAAMA24uLidPXVV8vpbPgSZsiQISRxAFo1EjkAAGAriYmJcjgc9T7u8Xg0c+bMEEYEAKHH0EoAAGArubm5iomJqTFPrjqHw6GMjAzFx8eHNjAACCF65AAAgK1ER0dr7NixcrlctR5zOBwaPXo0SRyAVo9EDgAA2M706dPrXPDE6XRqxowZFkQEAKHF0EoAAGA7RUVFio6OVklJSY3jLpdLWVlZ6tWrl0WRAUBo0CMHAABsp2PHjpo4caI8Hk/lMafTqRtvvJEkDkCbQCIHAABsadq0aaqoqKhxLDEx0aJoACC0GFoJAABsqaysTNHR0SooKJAkRUREKCcnR1FRURZHBgAtjx45AABgSxEREZo8ebIiIiLkdrs1adIkkjgAbQaJHAAAsK2pU6eqrKxMXq/3/2/v7oOsqu87jr/PfdhH9omF3eVZCqLGqIgtNK1GEFsjqeiYkEYpUyeFDrRJ6lT/qBkZ2+hYm3HTTMoMSTCmmQwdmqWNSsCJo4CmCmg0boSoPBkX5GGXhX2A3WXvw+kf33ty7z5ddpfde+7d/bxm7px7zzn3nO+9MrP34++J+++/3+9yREQyRl0rRUREJGfFYjFqamro6uqiqamJgoICv0sSEcmIkN8FiIiIiAxXMBhk5cqVtLa2KsSJyLiiICciIiLZJdIGsS6InodIO7hRiHXavlTRDohf5L4lVbRfKIWGOgjkQai453mBfAgVgROEcKkdDxZAWOPpRCR3qWuliIiIjDw3Bp2noPM4dDXCxbPQ3Zyybbbj3U0W1iLtEO/uG9YGcysX4i4EhzPyP5hvQS9cYo+8SVBYA/mTIG8i5Fcmt/mToWgGFFaDo/8XLiL+UpATERGRobvYDOePwPmjcP4j6DwBFz6GjmPQcRy6z4IbT57vBBMPx/bHI/7VPhiBMDgBS4luzB4ex7FwVzQNimZC8SwonALFs6FkDkyYY0FQRGQUKciJiIhI/6Id0PZbaHkP2g7C+cPQ9gGc/511ewQLO04YcK1FbTwK5Fm4i0eS4TVUBMVXQNk1MGEulFwJ5ddB2acgNMHXckVkbFCQExEREWg/BOfetdDW8hs492trXXPdREtaCNxIz1Y2GYSAte650USrnmMteRU3Qvn1Fu4q5kPpPDsmIjJICnIiIiLjTaTVAlvT69C4G5r2QqTFjgVCia6E+nkwqhzHxubFuoG4teCVXw+VC2HyzVD1WSio9rtKEcliCnIiIiJjXddpOL0TTu2EUy/BhQbbH8xLdAfUT4Gs4DjWTdXrolo0DWr+DGqWQvVSG4cnIpKgICciIjLWxDrh1Ctw6mU4+SK0HQIca20br+PYcpUTTATtuE2mMu3zUH0bTPnzvsssiMi4oiAnIiIyFnSfs+B2/Dk4/jOIdibGtUX9rkxGUiDP/ps6QeuCOf1umPkltdaJjEMKciIiIrkq0gYfb4GPNsOZ18EFAg7EFd7GBSeATZDi2ti62atg1n2QV+F3ZSKSAQpyIiIiOcWF06/C0R9CQ10itLmaTXLccxJLQQRg+r0wd7V1wXSGs0q6iOQCBTkREZFcEL0AhzfBB9+2ZQEC4exfVFv84YRtqYjCqXD1gzB3LYRL/K5KREaYgpyIiEg2u9gMBzfAB/9ui3C7Mb8rklziBCFYAFc9CFd9HQqq/K5IREaIgpyIiEg2il6AA09aC5wb1bg3uTyBMOBYmPv0egiX+l2RiFwmBTkREZFs01AHv/oH6G5SgJORFQhBuBwWfBtm/xU2WYqI5CIFORERkWzR8QnsWQWnd9kkFZrAREaD49gMp5UL4eYtUHyF3xWJyDAoyImIiGSD07vhl1+EaJsmMZHMCIQgUAi3/BSmfM7vakRkiDQnrYiIiN/er4VXbodIi0KcZE48ahPo7F4G+5/wuxoRGSK1yImIiPjpvX+G/d8E/TkWXzlwzcNw47f8LkREBklBTkRExC9HfwR7v+J3FSLGceCm78K8r/pdiYgMgrpWioiI+KH9ELy51u8qfq+xDWp3+F1F/2p3QGvH0N+39zCsexaclfCF78Aj/w3La0e+vjHDdeHtB6HlN35XIiKDoCAnIiLih199DZs60H+NbfDYVrjrxuS++gYLQN5j3bPDu3ZrhwWqTbvSh6ht79jx5bX2PNXtn4ZVG63Owdp5AD7zGDxyN7ib7fVTL/S99kho7bDvaCD1DcnPn+68TbvSH88MB976e7+LEJFBUJATERHJtLYP4eRLWTGxSWsHrN4Ef/1ZmDcluf/NIz3PWzZ/eNd/ejts/zX87TMDh6gteyzE/GSdPXa8a689N8yEb9xtdQ62Za5un21nVtr23Kbh1T8Yr30w8LHaHbC+DmrKYMMDFir7U99g35Hv3Cg0/R+cHYXEKyIjKuR3ASIiIuPOie029XsWBLlndltQ+uO5PffXlA0cOobi8RW2feK5/o83NMN9G2DPv0BZke1bezvMfwQWzrHawOqbVmH1PrTs0vf93iuXXfqgtHb0DJ2p1j0Lk0osnHqfbaBrbN03OvUNSyAPPtkGExf4XYmIpKEWORERkUxr2Z8Vi303tsHDm2HJp3rub2i2boDr66xb5Gh646Btp1Yk900pt23vVsEVi6zedF0sva6gA73urbXDWgS98zbt6nt9L6x556yvS57z9PZkS2PqvdbX2fbxFelDHFg4/dod6c/JqHhE4+REcoCCnIiISKZF27MiyO1LhLS5NT33139s2yees3Fmy2uHNj5tKF5937ZeF0iAqlLb9u6K6dW5L024dDf3bEns/bq3VRuhvcvOOb3R7tm7C+c/bbFuj6c3wsffte/lsa12zGtxTL1XfYOds2x+MgAur7Vxer3tPAB/Oi/5mbODa2saikhWU5ATERHJtIJqCIT9ruL3LV6pIQrgrgXQsgne/Vd49B4LN8+/PTo1pOsC2TvIlRXa9uCpkbn3zgN2j7tvstdVpTYWb9s78GJ98rxJJbB2qR33vqt0db+837YzJ8GaJfZdTquApU/2bOFsbIMjjX27tfrOCULhdL+rEJFLUJATERHJtEmfsUklfDbQuDWw7oA3zLQWpx+sHp3ZHofK66L48AiM3YPkhCiprWHXTLXtf72R3Pf4Ctj4FetyOpglGrz6vPF9ZUU27g/gx68lz3v+bQt6Wcdx7N+oiGQ1BTkREZFMm/YXECzwu4pB+9Ki0Qtyd6WZT2Pt0tG5p6e/VjUvLPb+vJt2wVf/s+cSDUPhhTrvntvegTuuH961Rp8DM+71uwgRuQQFORERkUwLl8HV/whObkweXVY0eqHKC3KpY/Aamm27YPbo3DPdvT2pn3fLHhsjt+GBnks0DMR7b39LJXj3XF4Ls77ec60+j69ryTkhmPd3UFDlYxEiMhgKciIiIn649htQPMuWIfDJ04nAcKm12Vo7bMbI0eC1Sh1tTO47ca7nsd4evWdk7n3/n/S9t/ddpH7e+zbYtvdYwoF47/3dmb7X9e7pTYyS+vCMxLIPw+KEbPzmdd/0qQARGQoFORERET8EC2Hxz62LpRP0pYR5iVkgWzuT+7bs6Tm7YkOzLXh927U931u7w1qO6hsufZ/UoNg7NM6stDF4P37NjrV22PMfrO4bnLyWuoVz0t8vtaaDJ22b2urmPb/zBmshe/L55L4X661FLfXzeq1oDc3J66VeJ7Vlr3aHvffRe3ouU/DTfXbel7N16JkTtAl4Fm+HcFZNoSkiA1CQExER8Uvp1XDbK4kwl/mWuUWJ2RK9FjCA4nybXdFbL+3chf7HsbVcsMDjrZc2EGcllK9Jvi5f07fr4JolNlV/+RpbDmDFov4nAfHqXJRmlkdnpS0m7rnqYdtXvS65z3teVgTPrLHPV70uWddTX+55TW+JgU07obzYQtrapdAV6Xn8P34Bq25O7ut93Z+sIzs5IVsEfMkvoOIGv6sRkUFyXNd1/S5CRERkXGs/CK8uh/YjGZ/N0puF8aFlw3v/8lp44aGRqyed9XUWpIZbq/QjEIbCKXDrNijP2tlXRKQfapETERHxW8k8uOMtmH4Xmf7TvHqxLcq9N80i2wPZe9jWXcuE+gZ7rF6cmfuNC04AqpfCne8qxInkIAU5ERGRbBAugVv+BxZ9H/LKwcnMguFe98Innx/ceDfPzgMwsTgzi1kfPAnfe9nq9JYHkMsQCEFoAtz0HRsTl1fhd0UiMgzqWikiIpJtus9C/aNw+Ps2CUU8Muq3bO2AZ3ZnZ7fF2h029qxKc3BcHicExOAPHoD5/wb5k/2uSEQug4KciIhItjpXD+89Bp+8YC108W6/K5JcFAhBPApTPwfXPw4T/9DvikRkBCjIiYiIZLu29+G3T8FHm21cUwZa6GQMCIQhHoNZfwnXPgLl1/ldkYiMIAU5ERGRXNFxDA5thCM/hK7GxA91hTpJEQiBG4NwBcz5G7hyHUyY7XdVIjIKFORERERyjRuH0zvh6I+gYav9cMe1/TL+OEHAAeJQtRiuXAvT77GgLyJjloKciIhILrt4Bo79DI5thVO7LNQFQhpPN9Y5QcAFHKi+FWZ8EWZ8AQqq/K5MRDJEQU5ERGSsiLTBiRfh2P/CiZ9DtAMCeeBGQH/uc5yT6ErbDcFCmHonzLgXpn7elqsQkXFHQU5ERGQscqPQ/JZ1wTzxEjTvsfF0gbzEuDr9+c96gTwLbk4IKv8IptwBNUuhcpG6TYqIgpyIiMi4EOuEpjeg8VVoeh2a90H0gnXRc0IQv+h3heOb11XSjUOoCCpugqpboOpWmHyz7RMRSaEgJyIiMi650PYhnNkHzW9C0y+h9X1rycOBYKLlThOojDAn2dKGawGu9CqYfAtMWgSVC6H0GltmQkQkDQU5ERERMW4U2g9By35oec8e596xZQ+8nwuBPHvuatmDtAJhwElOOuM4UDgNKhbYem7l10HZtRbi1E1SRIZBQU5ERETSi3VawGs/AucTj7YPof0gdJ5KLH8A1pIXtuF38SgwRlvznIB1RwULv16rpROAgmoomWcBrWQuTJhjj5K5ECr2r2YRGXMU5ERERGT44hFrses8AR3HofNk4vVJOH/U9l9stjDYmxNKjA2DRPpLBMAMCoSAAOAklmKLJbqX9hIsgPyJUDgdJlwBhVOhaAYU1Ni2cAoUz7QWSxGRDFCQExERkdEXv2iB7uJZ6E7dnoHuFlsqIdYJkVaItEOsA7rP2r5Yl10j0t5zzJ4b6xsQgwXJ1jIAHMgrtaeBfJs0JFyR2JZCuMzeEyq25/mTIL/SHnkTk9tgwah+PSIiQ6UgJyIiIiIikmM0JZKIiIiIiEiOUZATERERERHJMQpyIiIiIiIiOSYE1PldhIiIiIiIiAze/wMLb1oa0H0MLwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dezero import Variable, Model\n",
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "    \n",
    "x = Variable(np.random.randn(5, 10), name='x')\n",
    "model = TwoLayerNet(100, 10)\n",
    "model.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649353)\n",
      "variable(0.07888166506355147)\n",
      "variable(0.07655073683421637)\n",
      "variable(0.0763780308623822)\n",
      "variable(0.07618764131185572)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable, Model\n",
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "\n",
    "# 데이터셋\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "# 모델 정의\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "    \n",
    "model = TwoLayerNet(hidden_size, 1)\n",
    "\n",
    "# 학습 시작\n",
    "for i in range(iters):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.params():\n",
    "        p.data -= lr * p.grad.data\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "class MLP(Model):\n",
    "    def __init__(self, fc_output_sizes, activation=F.sigmoid):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "\n",
    "        for i, out_size in enumerate(fc_output_sizes):\n",
    "            layer = L.Linear(out_size)\n",
    "            setattr(self, 'l' + str(i), layer)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers[:-1]:\n",
    "            x = self.activation(l(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP((10, 1)) # 2층\n",
    "model = MLP((10, 20, 30, 40, 1)) # 5층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP46. Optimizer로 수행하는 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\tdef __init__(self):\n",
    "\t\tself.target = None\n",
    "\t\tself.hooks = []\n",
    "\t\t\n",
    "\tdef setup(self, target):\n",
    "\t\tself.target = target\n",
    "\t\treturn self\n",
    "\t\t\n",
    "\tdef update(self):\n",
    "\t\t# None 이외의 매개변수를 리스트에 모아둠\n",
    "\t\tparams = [p for p in self.target.params() if p.rad is not None]\n",
    "\t\t\n",
    "        # 전처리(옵션)\n",
    "\t\tfor f in self.hooks:\n",
    "\t\t    f(params)\n",
    "\t\t\n",
    "        # 매개변수 갱신\n",
    "\t\tfor param in params:\n",
    "\t\t\tself.update_one(param)\n",
    "\t\t\t\n",
    "\tdef update_one(self, param):\n",
    "\t\traise NotImplementedError()\n",
    "\t\t\n",
    "\tdef add_hook(self, f):\n",
    "\t\tself.hooks.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "\tdef __init__(self, lr = 0.01):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.lr = lr\n",
    "\t\t\n",
    "\tdef update_one(self, param):\n",
    "\t\tparam.data -= self.lr * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649353)\n",
      "variable(0.07888166506355147)\n",
      "variable(0.07655073683421637)\n",
      "variable(0.0763780308623822)\n",
      "variable(0.07618764131185572)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.SGD(lr)\n",
    "optimizer.setup(model)\n",
    "\n",
    "# 또는 다음처럼 한 줄로 합칠 수 있다.\n",
    "# optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "\n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            xp = cuda.get_array_module(param.data)\n",
    "            self.vs[v_key] = np.zeros_like(param.data)\n",
    "\n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data\n",
    "        param.data += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP47. 소프트맥스 함수와 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([4 5 6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.get_item(x, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0 0 0]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 2 3]\n",
      "          [1 2 3]\n",
      "          [4 5 6]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "indices = np.array([0, 0, 1])\n",
    "y = F.get_item(x, indices)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([4 5 6])\n",
      "variable([3 6])\n"
     ]
    }
   ],
   "source": [
    "Variable.__getitem__ = F.get_item\n",
    "\n",
    "y = x[1]\n",
    "print(y)\n",
    "\n",
    "y = x[:, 2]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.models import MLP\n",
    "model = MLP((10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.25337848 0.30853735 0.38821423]])\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.2, -0.4]])\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[-0.51222398 -0.48154886 -0.19620913]])\n",
      "variable([[0.29387554 0.30302989 0.40309457]])\n"
     ]
    }
   ],
   "source": [
    "model = MLP((10, 3))\n",
    "x = np.array([[0.2, -0.4]])\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_simple(x, axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis=axis, keepdims=True)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    p = softmax(x)\n",
    "    p = clip(p, 1e-15, 1.0)  # To avoid log(0)\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(1.005250977723018)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.2, -0.4], [0.3, 0.5], [1.3, -3.2], [2.1, 0.3]])\n",
    "t = np.array([2, 0, 1, 0])\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy_simple(y, t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP48. 다중 클래스 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n",
      "[-0.12995958 -0.00324155] 1\n",
      "[ 0.3282343  -0.54941994] 0\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "print(x.shape)\n",
    "print(t.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# ( 1 ) Hyperparameters\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "\n",
    "# ( 2 ) 데이터 읽기 / 모델, 옵티마이저 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD().setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.16\n",
      "epoch 2, loss 1.15\n",
      "epoch 3, loss 1.15\n",
      "epoch 4, loss 1.15\n",
      "epoch 5, loss 1.15\n",
      "epoch 6, loss 1.14\n",
      "epoch 7, loss 1.14\n",
      "epoch 8, loss 1.14\n",
      "epoch 9, loss 1.14\n",
      "epoch 10, loss 1.14\n",
      "epoch 11, loss 1.13\n",
      "epoch 12, loss 1.13\n",
      "epoch 13, loss 1.13\n",
      "epoch 14, loss 1.13\n",
      "epoch 15, loss 1.13\n",
      "epoch 16, loss 1.13\n",
      "epoch 17, loss 1.13\n",
      "epoch 18, loss 1.12\n",
      "epoch 19, loss 1.12\n",
      "epoch 20, loss 1.12\n",
      "epoch 21, loss 1.12\n",
      "epoch 22, loss 1.12\n",
      "epoch 23, loss 1.12\n",
      "epoch 24, loss 1.12\n",
      "epoch 25, loss 1.11\n",
      "epoch 26, loss 1.11\n",
      "epoch 27, loss 1.11\n",
      "epoch 28, loss 1.11\n",
      "epoch 29, loss 1.11\n",
      "epoch 30, loss 1.11\n",
      "epoch 31, loss 1.11\n",
      "epoch 32, loss 1.11\n",
      "epoch 33, loss 1.10\n",
      "epoch 34, loss 1.10\n",
      "epoch 35, loss 1.10\n",
      "epoch 36, loss 1.10\n",
      "epoch 37, loss 1.10\n",
      "epoch 38, loss 1.10\n",
      "epoch 39, loss 1.10\n",
      "epoch 40, loss 1.10\n",
      "epoch 41, loss 1.10\n",
      "epoch 42, loss 1.09\n",
      "epoch 43, loss 1.09\n",
      "epoch 44, loss 1.09\n",
      "epoch 45, loss 1.09\n",
      "epoch 46, loss 1.09\n",
      "epoch 47, loss 1.09\n",
      "epoch 48, loss 1.09\n",
      "epoch 49, loss 1.09\n",
      "epoch 50, loss 1.09\n",
      "epoch 51, loss 1.08\n",
      "epoch 52, loss 1.08\n",
      "epoch 53, loss 1.08\n",
      "epoch 54, loss 1.08\n",
      "epoch 55, loss 1.08\n",
      "epoch 56, loss 1.08\n",
      "epoch 57, loss 1.08\n",
      "epoch 58, loss 1.08\n",
      "epoch 59, loss 1.08\n",
      "epoch 60, loss 1.07\n",
      "epoch 61, loss 1.07\n",
      "epoch 62, loss 1.07\n",
      "epoch 63, loss 1.07\n",
      "epoch 64, loss 1.07\n",
      "epoch 65, loss 1.07\n",
      "epoch 66, loss 1.07\n",
      "epoch 67, loss 1.07\n",
      "epoch 68, loss 1.07\n",
      "epoch 69, loss 1.07\n",
      "epoch 70, loss 1.06\n",
      "epoch 71, loss 1.06\n",
      "epoch 72, loss 1.06\n",
      "epoch 73, loss 1.06\n",
      "epoch 74, loss 1.06\n",
      "epoch 75, loss 1.06\n",
      "epoch 76, loss 1.06\n",
      "epoch 77, loss 1.06\n",
      "epoch 78, loss 1.06\n",
      "epoch 79, loss 1.06\n",
      "epoch 80, loss 1.05\n",
      "epoch 81, loss 1.05\n",
      "epoch 82, loss 1.05\n",
      "epoch 83, loss 1.05\n",
      "epoch 84, loss 1.05\n",
      "epoch 85, loss 1.05\n",
      "epoch 86, loss 1.05\n",
      "epoch 87, loss 1.05\n",
      "epoch 88, loss 1.05\n",
      "epoch 89, loss 1.05\n",
      "epoch 90, loss 1.04\n",
      "epoch 91, loss 1.04\n",
      "epoch 92, loss 1.04\n",
      "epoch 93, loss 1.04\n",
      "epoch 94, loss 1.04\n",
      "epoch 95, loss 1.04\n",
      "epoch 96, loss 1.04\n",
      "epoch 97, loss 1.04\n",
      "epoch 98, loss 1.04\n",
      "epoch 99, loss 1.04\n",
      "epoch 100, loss 1.04\n",
      "epoch 101, loss 1.03\n",
      "epoch 102, loss 1.03\n",
      "epoch 103, loss 1.03\n",
      "epoch 104, loss 1.03\n",
      "epoch 105, loss 1.03\n",
      "epoch 106, loss 1.03\n",
      "epoch 107, loss 1.03\n",
      "epoch 108, loss 1.03\n",
      "epoch 109, loss 1.03\n",
      "epoch 110, loss 1.03\n",
      "epoch 111, loss 1.03\n",
      "epoch 112, loss 1.02\n",
      "epoch 113, loss 1.02\n",
      "epoch 114, loss 1.02\n",
      "epoch 115, loss 1.02\n",
      "epoch 116, loss 1.02\n",
      "epoch 117, loss 1.02\n",
      "epoch 118, loss 1.02\n",
      "epoch 119, loss 1.02\n",
      "epoch 120, loss 1.02\n",
      "epoch 121, loss 1.02\n",
      "epoch 122, loss 1.02\n",
      "epoch 123, loss 1.02\n",
      "epoch 124, loss 1.02\n",
      "epoch 125, loss 1.01\n",
      "epoch 126, loss 1.01\n",
      "epoch 127, loss 1.01\n",
      "epoch 128, loss 1.01\n",
      "epoch 129, loss 1.01\n",
      "epoch 130, loss 1.01\n",
      "epoch 131, loss 1.01\n",
      "epoch 132, loss 1.01\n",
      "epoch 133, loss 1.01\n",
      "epoch 134, loss 1.01\n",
      "epoch 135, loss 1.01\n",
      "epoch 136, loss 1.01\n",
      "epoch 137, loss 1.00\n",
      "epoch 138, loss 1.00\n",
      "epoch 139, loss 1.00\n",
      "epoch 140, loss 1.00\n",
      "epoch 141, loss 1.00\n",
      "epoch 142, loss 1.00\n",
      "epoch 143, loss 1.00\n",
      "epoch 144, loss 1.00\n",
      "epoch 145, loss 1.00\n",
      "epoch 146, loss 1.00\n",
      "epoch 147, loss 1.00\n",
      "epoch 148, loss 1.00\n",
      "epoch 149, loss 0.99\n",
      "epoch 150, loss 0.99\n",
      "epoch 151, loss 0.99\n",
      "epoch 152, loss 0.99\n",
      "epoch 153, loss 0.99\n",
      "epoch 154, loss 0.99\n",
      "epoch 155, loss 0.99\n",
      "epoch 156, loss 0.99\n",
      "epoch 157, loss 0.99\n",
      "epoch 158, loss 0.99\n",
      "epoch 159, loss 0.99\n",
      "epoch 160, loss 0.99\n",
      "epoch 161, loss 0.99\n",
      "epoch 162, loss 0.99\n",
      "epoch 163, loss 0.98\n",
      "epoch 164, loss 0.98\n",
      "epoch 165, loss 0.98\n",
      "epoch 166, loss 0.98\n",
      "epoch 167, loss 0.98\n",
      "epoch 168, loss 0.98\n",
      "epoch 169, loss 0.98\n",
      "epoch 170, loss 0.98\n",
      "epoch 171, loss 0.98\n",
      "epoch 172, loss 0.98\n",
      "epoch 173, loss 0.98\n",
      "epoch 174, loss 0.98\n",
      "epoch 175, loss 0.98\n",
      "epoch 176, loss 0.98\n",
      "epoch 177, loss 0.97\n",
      "epoch 178, loss 0.97\n",
      "epoch 179, loss 0.97\n",
      "epoch 180, loss 0.97\n",
      "epoch 181, loss 0.97\n",
      "epoch 182, loss 0.97\n",
      "epoch 183, loss 0.97\n",
      "epoch 184, loss 0.97\n",
      "epoch 185, loss 0.97\n",
      "epoch 186, loss 0.97\n",
      "epoch 187, loss 0.97\n",
      "epoch 188, loss 0.97\n",
      "epoch 189, loss 0.97\n",
      "epoch 190, loss 0.97\n",
      "epoch 191, loss 0.97\n",
      "epoch 192, loss 0.96\n",
      "epoch 193, loss 0.96\n",
      "epoch 194, loss 0.96\n",
      "epoch 195, loss 0.96\n",
      "epoch 196, loss 0.96\n",
      "epoch 197, loss 0.96\n",
      "epoch 198, loss 0.96\n",
      "epoch 199, loss 0.96\n",
      "epoch 200, loss 0.96\n",
      "epoch 201, loss 0.96\n",
      "epoch 202, loss 0.96\n",
      "epoch 203, loss 0.96\n",
      "epoch 204, loss 0.96\n",
      "epoch 205, loss 0.96\n",
      "epoch 206, loss 0.96\n",
      "epoch 207, loss 0.95\n",
      "epoch 208, loss 0.95\n",
      "epoch 209, loss 0.95\n",
      "epoch 210, loss 0.95\n",
      "epoch 211, loss 0.95\n",
      "epoch 212, loss 0.95\n",
      "epoch 213, loss 0.95\n",
      "epoch 214, loss 0.95\n",
      "epoch 215, loss 0.95\n",
      "epoch 216, loss 0.95\n",
      "epoch 217, loss 0.95\n",
      "epoch 218, loss 0.95\n",
      "epoch 219, loss 0.95\n",
      "epoch 220, loss 0.95\n",
      "epoch 221, loss 0.95\n",
      "epoch 222, loss 0.94\n",
      "epoch 223, loss 0.94\n",
      "epoch 224, loss 0.94\n",
      "epoch 225, loss 0.94\n",
      "epoch 226, loss 0.94\n",
      "epoch 227, loss 0.94\n",
      "epoch 228, loss 0.94\n",
      "epoch 229, loss 0.94\n",
      "epoch 230, loss 0.94\n",
      "epoch 231, loss 0.94\n",
      "epoch 232, loss 0.94\n",
      "epoch 233, loss 0.94\n",
      "epoch 234, loss 0.94\n",
      "epoch 235, loss 0.94\n",
      "epoch 236, loss 0.94\n",
      "epoch 237, loss 0.94\n",
      "epoch 238, loss 0.94\n",
      "epoch 239, loss 0.94\n",
      "epoch 240, loss 0.93\n",
      "epoch 241, loss 0.93\n",
      "epoch 242, loss 0.93\n",
      "epoch 243, loss 0.93\n",
      "epoch 244, loss 0.93\n",
      "epoch 245, loss 0.93\n",
      "epoch 246, loss 0.93\n",
      "epoch 247, loss 0.93\n",
      "epoch 248, loss 0.93\n",
      "epoch 249, loss 0.93\n",
      "epoch 250, loss 0.93\n",
      "epoch 251, loss 0.93\n",
      "epoch 252, loss 0.93\n",
      "epoch 253, loss 0.93\n",
      "epoch 254, loss 0.93\n",
      "epoch 255, loss 0.93\n",
      "epoch 256, loss 0.93\n",
      "epoch 257, loss 0.92\n",
      "epoch 258, loss 0.92\n",
      "epoch 259, loss 0.92\n",
      "epoch 260, loss 0.92\n",
      "epoch 261, loss 0.92\n",
      "epoch 262, loss 0.92\n",
      "epoch 263, loss 0.92\n",
      "epoch 264, loss 0.92\n",
      "epoch 265, loss 0.92\n",
      "epoch 266, loss 0.92\n",
      "epoch 267, loss 0.92\n",
      "epoch 268, loss 0.92\n",
      "epoch 269, loss 0.92\n",
      "epoch 270, loss 0.92\n",
      "epoch 271, loss 0.92\n",
      "epoch 272, loss 0.92\n",
      "epoch 273, loss 0.92\n",
      "epoch 274, loss 0.92\n",
      "epoch 275, loss 0.91\n",
      "epoch 276, loss 0.91\n",
      "epoch 277, loss 0.91\n",
      "epoch 278, loss 0.91\n",
      "epoch 279, loss 0.91\n",
      "epoch 280, loss 0.91\n",
      "epoch 281, loss 0.91\n",
      "epoch 282, loss 0.91\n",
      "epoch 283, loss 0.91\n",
      "epoch 284, loss 0.91\n",
      "epoch 285, loss 0.91\n",
      "epoch 286, loss 0.91\n",
      "epoch 287, loss 0.91\n",
      "epoch 288, loss 0.91\n",
      "epoch 289, loss 0.91\n",
      "epoch 290, loss 0.91\n",
      "epoch 291, loss 0.91\n",
      "epoch 292, loss 0.91\n",
      "epoch 293, loss 0.91\n",
      "epoch 294, loss 0.90\n",
      "epoch 295, loss 0.90\n",
      "epoch 296, loss 0.90\n",
      "epoch 297, loss 0.90\n",
      "epoch 298, loss 0.90\n",
      "epoch 299, loss 0.90\n",
      "epoch 300, loss 0.90\n"
     ]
    }
   ],
   "source": [
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # ( 3 ) Shuffle index for data\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # ( 4 ) 미니배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        # ( 5 ) 기울기 산출 / 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy_simple(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # ( 6 ) Print loss every epoch\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP49. Dataset 클래스와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]), \\\n",
    "                self.target_transform(self.label[index])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spiral(Dataset):\n",
    "    def prepare(self):\n",
    "        self.data, self.label = get_spiral(self.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import dezero.datasets as ds\n",
    "\n",
    "train_set = ds.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigData(Dataset):\n",
    "    def __getitem__(self, index):\n",
    "        x = np.load('data/{}.npy'.format(index))\n",
    "        t = np.load('label/{}.npy'.format(index))\n",
    "        return x, t\n",
    "    \n",
    "    def __len__():\n",
    "        return 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "train_set = ds.Spiral()\n",
    "\n",
    "batch_index = [0, 1, 2]\n",
    "batch = [train_set[i] for i in batch_index]\n",
    "\n",
    "x = np.array([example[0] for example in batch])\n",
    "t = np.array([example[1] for example in batch])\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.07\n",
      "epoch 2, loss 0.06\n",
      "epoch 3, loss 0.05\n",
      "epoch 4, loss 0.04\n",
      "epoch 5, loss 0.04\n",
      "epoch 6, loss 0.03\n",
      "epoch 7, loss 0.02\n",
      "epoch 8, loss 0.02\n",
      "epoch 9, loss 0.02\n",
      "epoch 10, loss 0.01\n",
      "epoch 11, loss 0.01\n",
      "epoch 12, loss 0.01\n",
      "epoch 13, loss 0.01\n",
      "epoch 14, loss 0.01\n",
      "epoch 15, loss 0.01\n",
      "epoch 16, loss 0.00\n",
      "epoch 17, loss 0.00\n",
      "epoch 18, loss 0.00\n",
      "epoch 19, loss 0.00\n",
      "epoch 20, loss 0.00\n",
      "epoch 21, loss 0.00\n",
      "epoch 22, loss 0.00\n",
      "epoch 23, loss 0.00\n",
      "epoch 24, loss 0.00\n",
      "epoch 25, loss 0.00\n",
      "epoch 26, loss 0.00\n",
      "epoch 27, loss 0.00\n",
      "epoch 28, loss 0.00\n",
      "epoch 29, loss 0.00\n",
      "epoch 30, loss 0.00\n",
      "epoch 31, loss 0.00\n",
      "epoch 32, loss 0.00\n",
      "epoch 33, loss 0.00\n",
      "epoch 34, loss 0.00\n",
      "epoch 35, loss 0.00\n",
      "epoch 36, loss 0.00\n",
      "epoch 37, loss 0.00\n",
      "epoch 38, loss 0.00\n",
      "epoch 39, loss 0.00\n",
      "epoch 40, loss 0.00\n",
      "epoch 41, loss 0.00\n",
      "epoch 42, loss 0.00\n",
      "epoch 43, loss 0.00\n",
      "epoch 44, loss 0.00\n",
      "epoch 45, loss 0.00\n",
      "epoch 46, loss 0.00\n",
      "epoch 47, loss 0.00\n",
      "epoch 48, loss 0.00\n",
      "epoch 49, loss 0.00\n",
      "epoch 50, loss 0.00\n",
      "epoch 51, loss 0.00\n",
      "epoch 52, loss 0.00\n",
      "epoch 53, loss 0.00\n",
      "epoch 54, loss 0.00\n",
      "epoch 55, loss 0.00\n",
      "epoch 56, loss 0.00\n",
      "epoch 57, loss 0.00\n",
      "epoch 58, loss 0.00\n",
      "epoch 59, loss 0.00\n",
      "epoch 60, loss 0.00\n",
      "epoch 61, loss 0.00\n",
      "epoch 62, loss 0.00\n",
      "epoch 63, loss 0.00\n",
      "epoch 64, loss 0.00\n",
      "epoch 65, loss 0.00\n",
      "epoch 66, loss 0.00\n",
      "epoch 67, loss 0.00\n",
      "epoch 68, loss 0.00\n",
      "epoch 69, loss 0.00\n",
      "epoch 70, loss 0.00\n",
      "epoch 71, loss 0.00\n",
      "epoch 72, loss 0.00\n",
      "epoch 73, loss 0.00\n",
      "epoch 74, loss 0.00\n",
      "epoch 75, loss 0.00\n",
      "epoch 76, loss 0.00\n",
      "epoch 77, loss 0.00\n",
      "epoch 78, loss 0.00\n",
      "epoch 79, loss 0.00\n",
      "epoch 80, loss 0.00\n",
      "epoch 81, loss 0.00\n",
      "epoch 82, loss 0.00\n",
      "epoch 83, loss 0.00\n",
      "epoch 84, loss 0.00\n",
      "epoch 85, loss 0.00\n",
      "epoch 86, loss 0.00\n",
      "epoch 87, loss 0.00\n",
      "epoch 88, loss 0.00\n",
      "epoch 89, loss 0.00\n",
      "epoch 90, loss 0.00\n",
      "epoch 91, loss 0.00\n",
      "epoch 92, loss 0.00\n",
      "epoch 93, loss 0.00\n",
      "epoch 94, loss 0.00\n",
      "epoch 95, loss 0.00\n",
      "epoch 96, loss 0.00\n",
      "epoch 97, loss 0.00\n",
      "epoch 98, loss 0.00\n",
      "epoch 99, loss 0.00\n",
      "epoch 100, loss 0.00\n",
      "epoch 101, loss 0.00\n",
      "epoch 102, loss 0.00\n",
      "epoch 103, loss 0.00\n",
      "epoch 104, loss 0.00\n",
      "epoch 105, loss 0.00\n",
      "epoch 106, loss 0.00\n",
      "epoch 107, loss 0.00\n",
      "epoch 108, loss 0.00\n",
      "epoch 109, loss 0.00\n",
      "epoch 110, loss 0.00\n",
      "epoch 111, loss 0.00\n",
      "epoch 112, loss 0.00\n",
      "epoch 113, loss 0.00\n",
      "epoch 114, loss 0.00\n",
      "epoch 115, loss 0.00\n",
      "epoch 116, loss 0.00\n",
      "epoch 117, loss 0.00\n",
      "epoch 118, loss 0.00\n",
      "epoch 119, loss 0.00\n",
      "epoch 120, loss 0.00\n",
      "epoch 121, loss 0.00\n",
      "epoch 122, loss 0.00\n",
      "epoch 123, loss 0.00\n",
      "epoch 124, loss 0.00\n",
      "epoch 125, loss 0.00\n",
      "epoch 126, loss 0.00\n",
      "epoch 127, loss 0.00\n",
      "epoch 128, loss 0.00\n",
      "epoch 129, loss 0.00\n",
      "epoch 130, loss 0.00\n",
      "epoch 131, loss 0.00\n",
      "epoch 132, loss 0.00\n",
      "epoch 133, loss 0.00\n",
      "epoch 134, loss 0.00\n",
      "epoch 135, loss 0.00\n",
      "epoch 136, loss 0.00\n",
      "epoch 137, loss 0.00\n",
      "epoch 138, loss 0.00\n",
      "epoch 139, loss 0.00\n",
      "epoch 140, loss 0.00\n",
      "epoch 141, loss 0.00\n",
      "epoch 142, loss 0.00\n",
      "epoch 143, loss 0.00\n",
      "epoch 144, loss 0.00\n",
      "epoch 145, loss 0.00\n",
      "epoch 146, loss 0.00\n",
      "epoch 147, loss 0.00\n",
      "epoch 148, loss 0.00\n",
      "epoch 149, loss 0.00\n",
      "epoch 150, loss 0.00\n",
      "epoch 151, loss 0.00\n",
      "epoch 152, loss 0.00\n",
      "epoch 153, loss 0.00\n",
      "epoch 154, loss 0.00\n",
      "epoch 155, loss 0.00\n",
      "epoch 156, loss 0.00\n",
      "epoch 157, loss 0.00\n",
      "epoch 158, loss 0.00\n",
      "epoch 159, loss 0.00\n",
      "epoch 160, loss 0.00\n",
      "epoch 161, loss 0.00\n",
      "epoch 162, loss 0.00\n",
      "epoch 163, loss 0.00\n",
      "epoch 164, loss 0.00\n",
      "epoch 165, loss 0.00\n",
      "epoch 166, loss 0.00\n",
      "epoch 167, loss 0.00\n",
      "epoch 168, loss 0.00\n",
      "epoch 169, loss 0.00\n",
      "epoch 170, loss 0.00\n",
      "epoch 171, loss 0.00\n",
      "epoch 172, loss 0.00\n",
      "epoch 173, loss 0.00\n",
      "epoch 174, loss 0.00\n",
      "epoch 175, loss 0.00\n",
      "epoch 176, loss 0.00\n",
      "epoch 177, loss 0.00\n",
      "epoch 178, loss 0.00\n",
      "epoch 179, loss 0.00\n",
      "epoch 180, loss 0.00\n",
      "epoch 181, loss 0.00\n",
      "epoch 182, loss 0.00\n",
      "epoch 183, loss 0.00\n",
      "epoch 184, loss 0.00\n",
      "epoch 185, loss 0.00\n",
      "epoch 186, loss 0.00\n",
      "epoch 187, loss 0.00\n",
      "epoch 188, loss 0.00\n",
      "epoch 189, loss 0.00\n",
      "epoch 190, loss 0.00\n",
      "epoch 191, loss 0.00\n",
      "epoch 192, loss 0.00\n",
      "epoch 193, loss 0.00\n",
      "epoch 194, loss 0.00\n",
      "epoch 195, loss 0.00\n",
      "epoch 196, loss 0.00\n",
      "epoch 197, loss 0.00\n",
      "epoch 198, loss 0.00\n",
      "epoch 199, loss 0.00\n",
      "epoch 200, loss 0.00\n",
      "epoch 201, loss 0.00\n",
      "epoch 202, loss 0.00\n",
      "epoch 203, loss 0.00\n",
      "epoch 204, loss 0.00\n",
      "epoch 205, loss 0.00\n",
      "epoch 206, loss 0.00\n",
      "epoch 207, loss 0.00\n",
      "epoch 208, loss 0.00\n",
      "epoch 209, loss 0.00\n",
      "epoch 210, loss 0.00\n",
      "epoch 211, loss 0.00\n",
      "epoch 212, loss 0.00\n",
      "epoch 213, loss 0.00\n",
      "epoch 214, loss 0.00\n",
      "epoch 215, loss 0.00\n",
      "epoch 216, loss 0.00\n",
      "epoch 217, loss 0.00\n",
      "epoch 218, loss 0.00\n",
      "epoch 219, loss 0.00\n",
      "epoch 220, loss 0.00\n",
      "epoch 221, loss 0.00\n",
      "epoch 222, loss 0.00\n",
      "epoch 223, loss 0.00\n",
      "epoch 224, loss 0.00\n",
      "epoch 225, loss 0.00\n",
      "epoch 226, loss 0.00\n",
      "epoch 227, loss 0.00\n",
      "epoch 228, loss 0.00\n",
      "epoch 229, loss 0.00\n",
      "epoch 230, loss 0.00\n",
      "epoch 231, loss 0.00\n",
      "epoch 232, loss 0.00\n",
      "epoch 233, loss 0.00\n",
      "epoch 234, loss 0.00\n",
      "epoch 235, loss 0.00\n",
      "epoch 236, loss 0.00\n",
      "epoch 237, loss 0.00\n",
      "epoch 238, loss 0.00\n",
      "epoch 239, loss 0.00\n",
      "epoch 240, loss 0.00\n",
      "epoch 241, loss 0.00\n",
      "epoch 242, loss 0.00\n",
      "epoch 243, loss 0.00\n",
      "epoch 244, loss 0.00\n",
      "epoch 245, loss 0.00\n",
      "epoch 246, loss 0.00\n",
      "epoch 247, loss 0.00\n",
      "epoch 248, loss 0.00\n",
      "epoch 249, loss 0.00\n",
      "epoch 250, loss 0.00\n",
      "epoch 251, loss 0.00\n",
      "epoch 252, loss 0.00\n",
      "epoch 253, loss 0.00\n",
      "epoch 254, loss 0.00\n",
      "epoch 255, loss 0.00\n",
      "epoch 256, loss 0.00\n",
      "epoch 257, loss 0.00\n",
      "epoch 258, loss 0.00\n",
      "epoch 259, loss 0.00\n",
      "epoch 260, loss 0.00\n",
      "epoch 261, loss 0.00\n",
      "epoch 262, loss 0.00\n",
      "epoch 263, loss 0.00\n",
      "epoch 264, loss 0.00\n",
      "epoch 265, loss 0.00\n",
      "epoch 266, loss 0.00\n",
      "epoch 267, loss 0.00\n",
      "epoch 268, loss 0.00\n",
      "epoch 269, loss 0.00\n",
      "epoch 270, loss 0.00\n",
      "epoch 271, loss 0.00\n",
      "epoch 272, loss 0.00\n",
      "epoch 273, loss 0.00\n",
      "epoch 274, loss 0.00\n",
      "epoch 275, loss 0.00\n",
      "epoch 276, loss 0.00\n",
      "epoch 277, loss 0.00\n",
      "epoch 278, loss 0.00\n",
      "epoch 279, loss 0.00\n",
      "epoch 280, loss 0.00\n",
      "epoch 281, loss 0.00\n",
      "epoch 282, loss 0.00\n",
      "epoch 283, loss 0.00\n",
      "epoch 284, loss 0.00\n",
      "epoch 285, loss 0.00\n",
      "epoch 286, loss 0.00\n",
      "epoch 287, loss 0.00\n",
      "epoch 288, loss 0.00\n",
      "epoch 289, loss 0.00\n",
      "epoch 290, loss 0.00\n",
      "epoch 291, loss 0.00\n",
      "epoch 292, loss 0.00\n",
      "epoch 293, loss 0.00\n",
      "epoch 294, loss 0.00\n",
      "epoch 295, loss 0.00\n",
      "epoch 296, loss 0.00\n",
      "epoch 297, loss 0.00\n",
      "epoch 298, loss 0.00\n",
      "epoch 299, loss 0.00\n",
      "epoch 300, loss 0.00\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral()\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 미니배치 꺼내기\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy_simple(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # 에포크마다 손실 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]), \\\n",
    "                self.target_transform(self.label[index])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x / 2.0\n",
    "    return y\n",
    "\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import transforms\n",
    "\n",
    "f = transforms.Normalize(mean=0.0, std=2.0)\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = transforms.Compose([transforms.Normalize(mean=0.0, std=2.0), \n",
    "                        transforms.AsType(np.float64)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
